{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load subject data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# define data root\n",
    "roamm_root = r\"/gpfs1/pi/anon/mindless_reading/ROAMM\"\n",
    "ml_data_root = os.path.join(roamm_root, 'subject_ml_data')\n",
    "# define subject id\n",
    "subject_id = 's10014'\n",
    "subject_dir = os.path.join(ml_data_root, subject_id, 'window_datasets')\n",
    "\n",
    "# load windowed data and labels\n",
    "window_size = 256\n",
    "data_file = os.path.join(subject_dir, f'{subject_id}_{window_size}windowed_data.npy')\n",
    "label_file = os.path.join(subject_dir, f'{subject_id}_{window_size}windowed_labels.npy')\n",
    "col_file = os.path.join(subject_dir, f'{subject_id}_col_names.npy')\n",
    "\n",
    "data = np.load(data_file)\n",
    "label = np.load(label_file)\n",
    "col_names = np.load(col_file)\n",
    "\n",
    "# extract EEG and eye-tracking features\n",
    "eeg_channel_num = 64\n",
    "eeg_data = data[:, :eeg_channel_num, :].astype(np.float32)\n",
    "\n",
    "eye_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# define data root\n",
    "# this is the path to the ROAMM folder on local machine\n",
    "roamm_root = r\"/gpfs1/pi/anon/mindless_reading/ROAMM\"\n",
    "ml_data_root = os.path.join(roamm_root, 'subject_ml_data')\n",
    "# define subject id\n",
    "subject_id = 's10014'\n",
    "subject_dir = os.path.join(ml_data_root, subject_id)\n",
    "# load all runs for a subject\n",
    "pkl_files = [f for f in os.listdir(subject_dir) if f.endswith('.pkl')]\n",
    "df = pd.DataFrame()\n",
    "for pkl_file in pkl_files:\n",
    "    df_sub_single_run = pd.read_pickle(os.path.join(subject_dir, pkl_file))\n",
    "    df_sub_single_run = df_sub_single_run[df_sub_single_run['first_pass_reading'] == 1]\n",
    "    # convert bool col explicitly to avoid pandas warning\n",
    "    for col in ['is_blink', 'is_saccade', 'is_fixation', 'is_mw', 'first_pass_reading']:\n",
    "        df_sub_single_run[col] = df_sub_single_run[col] == True\n",
    "    df = pd.concat([df, df_sub_single_run])\n",
    "\n",
    "# normalize pupil size features\n",
    "df['blink_interp_LPupil_norm'] = df['blink_interp_LPupil'] / df['blink_interp_LPupil'].median()\n",
    "df['blink_interp_RPupil_norm'] = df['blink_interp_RPupil'] / df['blink_interp_RPupil'].median()\n",
    "\n",
    "# define EEG and eye tracking features\n",
    "eeg_cols = df.columns.tolist()[:64]  #first 64 columns are EEG channels\n",
    "eye_cols = ['blink_interp_LX', 'blink_interp_LY', 'blink_interp_RX', 'blink_interp_RY','blink_interp_LPupil_norm', 'blink_interp_RPupil_norm']\n",
    "\n",
    "# Downsample data using 1-second windows (fs = 256 Hz)\n",
    "windowed_data = []\n",
    "windowed_labels = []\n",
    "window_size = 64\n",
    "\n",
    "# Process data in chunks of window_size\n",
    "for i in range(0, len(df), window_size):\n",
    "    window = df.iloc[i:i+window_size]\n",
    "    # Skip if window is too small\n",
    "    if len(window) < window_size:\n",
    "        continue\n",
    "    # Check if labels are consistent in this window\n",
    "    labels_in_window = window['is_mw'].unique()\n",
    "    if len(labels_in_window) > 1:\n",
    "        # Skip windows with mixed labels\n",
    "        continue\n",
    "\n",
    "    # Extract features for this window: keep as 2D array (window_size x feature_number)\n",
    "    eeg_features = window[eeg_cols].values  # shape: (window_size, 64)\n",
    "    eye_features = window[eye_cols].values  # shape: (window_size, len(eye_cols))\n",
    "    # Concatenate along the feature axis (axis=1), not the sample axis (axis=0)\n",
    "    features = np.concatenate([eeg_features, eye_features], axis=1)\n",
    "    windowed_data.append(features)\n",
    "    # Use the consistent label\n",
    "    windowed_labels.append(labels_in_window[0])\n",
    "\n",
    "# Handle class imbalance\n",
    "print(\"Handling class imbalance...\")\n",
    "# Use RandomUnderSampler on flattened data, then recover 3D structure\n",
    "windowed_data_flat = [w.flatten() for w in windowed_data]\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_resampled_flat, y_resampled = undersampler.fit_resample(windowed_data_flat, windowed_labels)\n",
    "# Recover 3D array: (n_samples, window_size, n_features)\n",
    "window_size = windowed_data[0].shape[0]\n",
    "n_features = windowed_data[0].shape[1]\n",
    "X_resampled = np.array(X_resampled_flat).reshape(-1, window_size, n_features)\n",
    "# print(f\"Class distribution:\\n{y_resampled.value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier on raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# First, let's try EEG features only\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== EEG Features Only ===\")\n",
    "print(\"=\"*50)\n",
    "X_eeg = X_resampled[eeg_cols].copy()\n",
    "y_eeg = y_resampled\n",
    "\n",
    "# Split the data\n",
    "X_eeg_train, X_eeg_test, y_eeg_train, y_eeg_test = train_test_split(\n",
    "    X_eeg, y_eeg, test_size=0.2, random_state=42, stratify=y_eeg\n",
    ")\n",
    "\n",
    "# Create pipeline with scaling and classifier\n",
    "eeg_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Train and evaluate EEG-only model\n",
    "eeg_pipeline.fit(X_eeg_train, y_eeg_train)\n",
    "y_eeg_pred = eeg_pipeline.predict(X_eeg_test)\n",
    "\n",
    "print(f\"EEG-only Accuracy: {accuracy_score(y_eeg_test, y_eeg_pred):.3f}\")\n",
    "print(\"\\nEEG-only Classification Report:\")\n",
    "print(classification_report(y_eeg_test, y_eeg_pred))\n",
    "print(\"\\nEEG-only Confusion Matrix:\")\n",
    "print(confusion_matrix(y_eeg_test, y_eeg_pred))\n",
    "\n",
    "# Next, let's try eye tracking features only\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== Eye Tracking Features Only ===\")\n",
    "print(\"=\"*50)\n",
    "X_eye = X_resampled[eye_cols].copy()\n",
    "y_eye = y_resampled\n",
    "\n",
    "# Split the data\n",
    "X_eye_train, X_eye_test, y_eye_train, y_eye_test = train_test_split(\n",
    "    X_eye, y_eye, test_size=0.2, random_state=42, stratify=y_eye\n",
    ")\n",
    "\n",
    "# Create pipeline with scaling and classifier\n",
    "eye_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Train and evaluate eye-only model\n",
    "eye_pipeline.fit(X_eye_train, y_eye_train)\n",
    "y_eye_pred = eye_pipeline.predict(X_eye_test)\n",
    "\n",
    "print(f\"Eye-only Accuracy: {accuracy_score(y_eye_test, y_eye_pred):.3f}\")\n",
    "print(\"\\nEye-only Classification Report:\")\n",
    "print(classification_report(y_eye_test, y_eye_pred))\n",
    "print(\"\\nEye-only Confusion Matrix:\")\n",
    "print(confusion_matrix(y_eye_test, y_eye_pred))\n",
    "\n",
    "# Now let's try combined features\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== Combined EEG + Eye Tracking Features ===\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled\n",
    ")\n",
    "\n",
    "# Create pipeline with scaling and classifier\n",
    "combined_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Train and evaluate combined model\n",
    "combined_pipeline.fit(X_train, y_train)\n",
    "y_pred = combined_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f\"Combined Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(\"\\nCombined Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nCombined Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== PERFORMANCE SUMMARY ===\")\n",
    "print(\"=\"*50)\n",
    "print(f\"EEG-only Accuracy:      {accuracy_score(y_eeg_test, y_eeg_pred):.3f}\")\n",
    "print(f\"Eye-only Accuracy:      {accuracy_score(y_eye_test, y_eye_pred):.3f}\")\n",
    "print(f\"Combined Accuracy:      {accuracy_score(y_test, y_pred):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from braindecode.models import EEGNet\n",
    "from braindecode.util import set_random_seeds\n",
    "\n",
    "# Prepare EEG data for EEGNet\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== EEGNet for Mind-Wandering Classification ===\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "eeg_data = X_resampled[:, :, :64]  \n",
    "eeg_data = np.transpose(eeg_data, (0, 2, 1))  # (N, num_channels, window_size)\n",
    "eeg_labels = y_resampled\n",
    "num_channels = eeg_data.shape[1]\n",
    "window_size = eeg_data.shape[2]\n",
    "\n",
    "# Add channel dimension for PyTorch Conv2d\n",
    "eeg_data_reshaped = eeg_data[:, np.newaxis, :, :]  # (N, 1, window_size, 64)\n",
    "\n",
    "# Split the data\n",
    "X_train_eegnet, X_test_eegnet, y_train_eegnet, y_test_eegnet = train_test_split(\n",
    "    eeg_data_reshaped, eeg_labels, test_size=0.2, random_state=42, stratify=eeg_labels\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_eegnet)\n",
    "y_train_tensor = torch.LongTensor(y_train_eegnet)\n",
    "X_test_tensor = torch.FloatTensor(X_test_eegnet)\n",
    "y_test_tensor = torch.LongTensor(y_test_eegnet)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize model\n",
    "# Set up GPU if it is there\n",
    "cuda = torch.cuda.is_available() \n",
    "device = \"cuda\" if cuda else \"cpu\"\n",
    "\n",
    "# Set random seed to be able to reproduce results\n",
    "seed = 42\n",
    "set_random_seeds(seed=seed, cuda=cuda)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# define model\n",
    "model = EEGNet(n_chans=64, n_outputs=2, n_times=window_size)\n",
    "# print(model)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, device, epochs=50):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            # Calculate accuracy on training set for this epoch\n",
    "            model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for data, target in train_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    outputs = model(data)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += target.size(0)\n",
    "                    correct += (predicted == target).sum().item()\n",
    "            train_acc = 100 * correct / total\n",
    "            model.train()\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Acc: {train_acc:.2f}%')\n",
    "    \n",
    "    return train_losses\n",
    "\n",
    "# Train the model\n",
    "print(\"Training EEGNet...\")\n",
    "train_losses = train_model(model, train_loader, criterion, optimizer, device, epochs=200)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy, all_predictions, all_targets\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy, y_pred_eegnet, y_true_eegnet = evaluate_model(model, test_loader, device)\n",
    "\n",
    "print(f\"\\nEEGNet Accuracy: {accuracy:.3f}%\")\n",
    "print(\"\\nEEGNet Classification Report:\")\n",
    "print(classification_report(y_true_eegnet, y_pred_eegnet))\n",
    "print(\"\\nEEGNet Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true_eegnet, y_pred_eegnet))\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses)\n",
    "plt.title('EEGNet Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Updated performance summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== UPDATED PERFORMANCE SUMMARY ===\")\n",
    "print(\"=\"*50)\n",
    "print(f\"EEGNet Accuracy:           {accuracy/100:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ShallowConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple ShallowConvNet architecture for EEG\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class ShallowConvNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_samples, n_classes):\n",
    "        super(ShallowConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 40, (1, 13), padding=(0, 6), bias=False)\n",
    "        self.conv2 = nn.Conv2d(40, 40, (n_channels, 1), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(40)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=(1, 35), stride=(1, 7))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.elu = nn.ELU()\n",
    "        self._n_samples = n_samples\n",
    "        self.fc = nn.Linear(self._get_flattened_size(n_channels, n_samples), n_classes)\n",
    "\n",
    "    def _get_flattened_size(self, n_channels, n_samples):\n",
    "        # Calculate the size after conv/pool for the FC layer\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1, 1, n_channels, n_samples)\n",
    "            x = self.conv1(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.elu(x)\n",
    "            x = self.pool(x)\n",
    "            x = self.dropout(x)\n",
    "            return x.view(1, -1).shape[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, 1, n_channels, n_samples)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Set parameters\n",
    "window_size = 256\n",
    "num_channels = 64\n",
    "num_classes = len(set(y_resampled))\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "# Prepare EEG data (ensure correct shape: samples, 1, channels, samples)\n",
    "eeg_data = X_resampled[:, :, :num_channels]  # shape: (N, window_size, num_channels)\n",
    "eeg_labels = y_resampled\n",
    "\n",
    "# Convert to torch tensors and reshape\n",
    "eeg_data_tensor = torch.tensor(eeg_data, dtype=torch.float32).permute(0, 2, 1).unsqueeze(1)  # (N, 1, C, T)\n",
    "eeg_labels_tensor = torch.tensor(eeg_labels, dtype=torch.long)\n",
    "\n",
    "# Split into train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    eeg_data_tensor, eeg_labels_tensor, test_size=0.2, random_state=42, stratify=eeg_labels_tensor\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Instantiate model, loss, optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ShallowConvNet(num_channels, window_size, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * data.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "    if (epoch+1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy, all_predictions, all_targets\n",
    "\n",
    "accuracy, y_pred_shallow, y_true_shallow = evaluate_model(model, test_loader, device)\n",
    "\n",
    "print(f\"\\nShallowConvNet Accuracy: {accuracy:.3f}%\")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"\\nShallowConvNet Classification Report:\")\n",
    "print(classification_report(y_true_shallow, y_pred_shallow))\n",
    "print(\"\\nShallowConvNet Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true_shallow, y_pred_shallow))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses)\n",
    "plt.title('ShallowConvNet Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== SHALLOW CONVNET PERFORMANCE SUMMARY ===\")\n",
    "print(\"=\"*50)\n",
    "print(f\"ShallowConvNet Accuracy:           {accuracy/100:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from braindecode.models import EEGNet\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare EEGNet data\n",
    "# Assume X_resampled: (N, window_size, num_channels), y_resampled: (N,)\n",
    "# EEGNet expects input shape: (batch, 1, n_chans, n_times)\n",
    "# So we need to transpose and add channel dimension\n",
    "\n",
    "# Set EEGNet parameters based on your data\n",
    "# Transpose to (N, n_chans, n_times)\n",
    "num_channels = 64\n",
    "eeg_data = X_resampled[:, :, :num_channels]  # (N, window_size, num_channels)\n",
    "eeg_data = np.transpose(eeg_data, (0, 2, 1))  # (N, num_channels, window_size)\n",
    "eeg_labels = y_resampled\n",
    "\n",
    "# Add channel dimension for PyTorch Conv2d: (N, 1, n_chans, n_times)\n",
    "eeg_data_reshaped = eeg_data[:, np.newaxis, :, :]  # (N, 1, n_chans, n_times)\n",
    "\n",
    "# Split the data\n",
    "X_train_eegnet, X_test_eegnet, y_train_eegnet, y_test_eegnet = train_test_split(\n",
    "    eeg_data_reshaped, eeg_labels, test_size=0.2, random_state=42, stratify=eeg_labels\n",
    ")\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train_eegnet = torch.tensor(X_train_eegnet, dtype=torch.float32)\n",
    "X_test_eegnet = torch.tensor(X_test_eegnet, dtype=torch.float32)\n",
    "y_train_eegnet = torch.tensor(y_train_eegnet, dtype=torch.long)\n",
    "y_test_eegnet = torch.tensor(y_test_eegnet, dtype=torch.long)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 32\n",
    "train_dataset_eegnet = TensorDataset(X_train_eegnet, y_train_eegnet)\n",
    "test_dataset_eegnet = TensorDataset(X_test_eegnet, y_test_eegnet)\n",
    "train_loader_eegnet = DataLoader(train_dataset_eegnet, batch_size=batch_size, shuffle=True)\n",
    "test_loader_eegnet = DataLoader(test_dataset_eegnet, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Set model parameters\n",
    "n_chans = X_train_eegnet.shape[2]  # number of EEG channels\n",
    "n_times = X_train_eegnet.shape[3]  # number of time samples\n",
    "n_outputs = len(np.unique(y_train_eegnet.numpy()))  # number of classes\n",
    "\n",
    "# Instantiate EEGNet model\n",
    "eegnet_model = EEGNet(\n",
    "    n_chans=n_chans,\n",
    "    n_outputs=n_outputs,\n",
    "    n_times=n_times,\n",
    "    F1=8,\n",
    "    D=2,\n",
    "    F2=None,  # default is F1*D\n",
    "    kernel_length=64,\n",
    "    depthwise_kernel_length=16,\n",
    "    pool1_kernel_size=4,\n",
    "    pool2_kernel_size=8,\n",
    "    final_conv_length=\"auto\",\n",
    "    drop_prob=0.25,\n",
    "    activation=nn.ELU,\n",
    "    batch_norm_momentum=0.01,\n",
    "    batch_norm_affine=True,\n",
    "    batch_norm_eps=1e-3,\n",
    "    conv_spatial_max_norm=1,\n",
    "    final_layer_with_constraint=False,\n",
    "    norm_rate=0.25,\n",
    ")\n",
    "\n",
    "eegnet_model = eegnet_model.to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(eegnet_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "def train_model_eegnet(model, train_loader, criterion, optimizer, device, epochs=200):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * data.size(0)\n",
    "        epoch_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        if (epoch+1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    return train_losses\n",
    "\n",
    "print(\"Training EEGNet...\")\n",
    "eegnet_train_losses = train_model_eegnet(eegnet_model, train_loader_eegnet, criterion, optimizer, device, epochs=200)\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model_eegnet(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy, all_predictions, all_targets\n",
    "\n",
    "eegnet_accuracy, y_pred_eegnet, y_true_eegnet = evaluate_model_eegnet(eegnet_model, test_loader_eegnet, device)\n",
    "\n",
    "print(f\"\\nEEGNet Accuracy: {eegnet_accuracy:.3f}%\")\n",
    "print(\"\\nEEGNet Classification Report:\")\n",
    "print(classification_report(y_true_eegnet, y_pred_eegnet))\n",
    "print(\"\\nEEGNet Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true_eegnet, y_pred_eegnet))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(eegnet_train_losses)\n",
    "plt.title('EEGNet Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== EEGNET PERFORMANCE SUMMARY ===\")\n",
    "print(\"=\"*50)\n",
    "print(f\"EEGNet Accuracy:           {eegnet_accuracy/100:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datasets import MOABBDataset\n",
    "\n",
    "subject_id = 3\n",
    "# Load the dataset\n",
    "dataset = BNCI2014_001()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from braindecode.preprocessing import (\n",
    "    Preprocessor,\n",
    "    exponential_moving_standardize,\n",
    "    preprocess,\n",
    ")\n",
    "\n",
    "low_cut_hz = 4.0  # low cut frequency for filtering\n",
    "high_cut_hz = 38.0  # high cut frequency for filtering\n",
    "# Parameters for exponential moving standardization\n",
    "factor_new = 1e-3\n",
    "init_block_size = 1000\n",
    "\n",
    "preprocessors = [\n",
    "    Preprocessor(\"pick_types\", eeg=True, meg=False, stim=False),  # Keep EEG sensors\n",
    "    Preprocessor(\n",
    "        lambda data, factor: np.multiply(data, factor),  # Convert from V to uV\n",
    "        factor=1e6,\n",
    "    ),\n",
    "    Preprocessor(\"filter\", l_freq=low_cut_hz, h_freq=high_cut_hz),  # Bandpass filter\n",
    "    Preprocessor(\n",
    "        exponential_moving_standardize,  # Exponential moving standardization\n",
    "        factor_new=factor_new,\n",
    "        init_block_size=init_block_size,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Preprocess the data\n",
    "preprocess(dataset, preprocessors, n_jobs=-1)\n",
    "\n",
    "from braindecode.preprocessing import create_windows_from_events\n",
    "\n",
    "trial_start_offset_seconds = -0.5\n",
    "# Extract sampling frequency, check that they are same in all datasets\n",
    "sfreq = dataset.datasets[0].raw.info[\"sfreq\"]\n",
    "assert all([ds.raw.info[\"sfreq\"] == sfreq for ds in dataset.datasets])\n",
    "# Calculate the window start offset in samples.\n",
    "trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n",
    "\n",
    "# Create windows using braindecode function for this. It needs parameters to\n",
    "# define how windows should be used.\n",
    "windows_dataset = create_windows_from_events(\n",
    "    dataset,\n",
    "    trial_start_offset_samples=trial_start_offset_samples,\n",
    "    trial_stop_offset_samples=0,\n",
    "    preload=True,\n",
    ")\n",
    "\n",
    "splitted = windows_dataset.split(\"session\")\n",
    "train_set = splitted[\"0train\"]  # Session train\n",
    "test_set = splitted[\"1test\"]  # Session evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codecs import raw_unicode_escape_decode\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import mne\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from skorch.callbacks import EarlyStopping, EpochScoring\n",
    "from skorch.dataset import ValidSplit\n",
    "from braindecode import EEGClassifier\n",
    "from braindecode.models import EEGNet\n",
    "from braindecode.util import set_random_seeds\n",
    "from skorch.callbacks import LRScheduler\n",
    "from braindecode.datasets import BaseConcatDataset, BaseDataset\n",
    "from braindecode.preprocessing import create_fixed_length_windows\n",
    "\n",
    "\n",
    "# Transpose to (N, n_chans, n_times)\n",
    "num_channels = 64\n",
    "eeg_data = X_resampled[:, :, :num_channels]  # (N, window_size, num_channels)\n",
    "eeg_data = np.transpose(eeg_data, (0, 2, 1))  # (N, num_channels, window_size)\n",
    "eeg_labels = np.array(y_resampled, dtype=int)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    eeg_data, eeg_labels, test_size=0.2, random_state=42, stratify=eeg_labels\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "sampling_freq = 256  # in Hertz\n",
    "info = mne.create_info(num_channels, sfreq=sampling_freq)\n",
    "datasets = []\n",
    "for i in range(len(X_train)):\n",
    "    raw = mne.io.RawArray(X_train[i], info, verbose=False)\n",
    "    description = pd.Series(data=[y_train[i], 'train'], index=[\"target\", \"session\"])\n",
    "    datasets.append(BaseDataset(raw, description, target_name=\"target\"))\n",
    "\n",
    "for j in range(len(X_test)):\n",
    "    raw = mne.io.RawArray(X_test[j], info, verbose=False)\n",
    "    description = pd.Series(data=[y_test[j], 'test'], index=[\"target\", \"session\"])\n",
    "    datasets.append(BaseDataset(raw, description, target_name=\"target\"))\n",
    "datasets = BaseConcatDataset(datasets)\n",
    "\n",
    "# Set up GPU if it is there\n",
    "cuda = torch.cuda.is_available() \n",
    "device = \"cuda\" if cuda else \"cpu\"\n",
    "\n",
    "# Set random seed to be able to reproduce results\n",
    "seed = 42\n",
    "set_random_seeds(seed=seed, cuda=cuda)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# define model\n",
    "model = EEGNet(n_chans=64, n_outputs=2, n_times=window_size)\n",
    "# print(model)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "# Splitting windowed data into train, valid and test subsets.\n",
    "splits = datasets.split(\"session\")\n",
    "train_set = splits[\"train\"]\n",
    "# valid_set = splits[\"valid\"]\n",
    "test_set = splits[\"test\"]\n",
    "\n",
    "# define training hyperparameters\n",
    "lr = 0.0625 * 0.1\n",
    "weight_decay = 0\n",
    "batch_size = 32\n",
    "n_epochs = 30\n",
    "patience = 3\n",
    "\n",
    "clf = EEGClassifier(\n",
    "    model,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    train_split=None,\n",
    "    optimizer__lr=lr,\n",
    "    optimizer__weight_decay=weight_decay,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[\n",
    "        \"accuracy\",\n",
    "        (\"lr_scheduler\", LRScheduler(\"CosineAnnealingLR\", T_max=n_epochs - 1)),\n",
    "    ],\n",
    "    verbose=1,  # Not printing the results for each epoch\n",
    "    device=device,\n",
    "    classes=list(range(2)),\n",
    "    max_epochs=n_epochs,\n",
    ")\n",
    "\n",
    "# Model training for a specified number of epochs. `y` is None as it is already supplied\n",
    "# in the dataset.\n",
    "# clf.fit(train_set, y=None)\n",
    "\n",
    "# # evaluated the model after training\n",
    "y_test_ = test_set.get_metadata().target\n",
    "# test_acc = clf.score(test_set, y=y_test)\n",
    "# print(f\"Test acc: {(test_acc * 100):.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Band Power Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import pandas as pd\n",
    "\n",
    "eeg_data = X_resampled[:, :, :64]  \n",
    "eeg_data = np.transpose(eeg_data, (0, 2, 1))  # (N, num_channels, window_size)\n",
    "\n",
    "def compute_band_power(data, fs=256, window_size=64):\n",
    "    \"\"\"\n",
    "    Compute band power for different frequency bands over 64-sample windows.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like, shape (n_samples, n_channels) or (n_channels, n_samples)\n",
    "        EEG data\n",
    "    fs : int, default=256\n",
    "        Sampling frequency in Hz\n",
    "    window_size : int, default=64\n",
    "        Window size for analysis\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    band_powers : dict\n",
    "        Dictionary containing power for each frequency band\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define frequency bands (in Hz)\n",
    "    bands = {\n",
    "        'delta': (1, 4),\n",
    "        'theta': (4, 8), \n",
    "        'alpha': (8, 13),\n",
    "        'beta': (13, 30),\n",
    "        'gamma': (30, 50)\n",
    "    }\n",
    "    \n",
    "    # Ensure data is in the right format (samples, channels)\n",
    "    if data.shape[0] < data.shape[1]:\n",
    "        data = data.T\n",
    "    \n",
    "    n_samples, n_channels = data.shape\n",
    "    n_windows = n_samples // window_size\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    band_powers = {band: np.zeros((n_windows, n_channels)) for band in bands.keys()}\n",
    "    \n",
    "    for i in range(n_windows):\n",
    "        start_idx = i * window_size\n",
    "        end_idx = start_idx + window_size\n",
    "        window_data = data[start_idx:end_idx, :]\n",
    "        \n",
    "        for ch in range(n_channels):\n",
    "            # Compute power spectral density using Welch's method\n",
    "            freqs, psd = signal.welch(window_data[:, ch], fs=fs, nperseg=window_size, \n",
    "                                    noverlap=window_size//2, nfft=window_size)\n",
    "            \n",
    "            # Compute band power for each frequency band\n",
    "            for band_name, (low_freq, high_freq) in bands.items():\n",
    "                # Find frequency indices for the band\n",
    "                freq_mask = (freqs >= low_freq) & (freqs <= high_freq)\n",
    "                # Compute power in the band (integrate PSD)\n",
    "                band_power = np.trapz(psd[freq_mask], freqs[freq_mask])\n",
    "                band_powers[band_name][i, ch] = band_power\n",
    "    \n",
    "    return band_powers, freqs, n_windows\n",
    "\n",
    "# Test with the existing EEG data\n",
    "print(\"Computing band powers for EEG data...\")\n",
    "print(f\"EEG data shape: {X_resampled.shape}\")\n",
    "\n",
    "# Extract EEG data (first 64 channels)\n",
    "eeg_data = X_resampled[:, :, :64]  # Shape: (n_samples, window_size, n_channels)\n",
    "\n",
    "# Take first sample for demonstration\n",
    "sample_eeg = eeg_data[0]  # Shape: (window_size, n_channels)\n",
    "print(f\"Sample EEG shape: {sample_eeg.shape}\")\n",
    "\n",
    "# Compute band powers\n",
    "band_powers, freqs, n_windows = compute_band_power(sample_eeg, fs=256, window_size=64)\n",
    "\n",
    "print(f\"\\nComputed band powers for {n_windows} windows\")\n",
    "print(\"Band power shapes:\")\n",
    "for band_name, powers in band_powers.items():\n",
    "    print(f\"  {band_name}: {powers.shape}\")\n",
    "    print(f\"  {band_name} mean power: {np.mean(powers):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute band powers for multiple samples and visualize results\n",
    "print(\"Computing band powers for multiple EEG samples...\")\n",
    "\n",
    "# Compute band powers for first 10 samples\n",
    "n_samples_to_analyze = min(10, len(eeg_data))\n",
    "all_band_powers = {band: [] for band in ['delta', 'theta', 'alpha', 'beta', 'gamma']}\n",
    "\n",
    "for i in range(n_samples_to_analyze):\n",
    "    sample_eeg = eeg_data[i]  # Shape: (window_size, n_channels)\n",
    "    band_powers, freqs, n_windows = compute_band_power(sample_eeg, fs=256, window_size=64)\n",
    "    \n",
    "    # Average across channels and windows for this sample\n",
    "    for band_name, powers in band_powers.items():\n",
    "        mean_power = np.mean(powers)\n",
    "        all_band_powers[band_name].append(mean_power)\n",
    "\n",
    "# Create DataFrame for easy analysis\n",
    "band_power_df = pd.DataFrame(all_band_powers)\n",
    "band_power_df.index.name = 'Sample'\n",
    "\n",
    "print(\"\\nBand power statistics across samples:\")\n",
    "print(band_power_df.describe())\n",
    "\n",
    "# Visualize band powers\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Band power distribution\n",
    "axes[0, 0].boxplot([all_band_powers[band] for band in all_band_powers.keys()], \n",
    "                   labels=list(all_band_powers.keys()))\n",
    "axes[0, 0].set_title('Band Power Distribution Across Samples')\n",
    "axes[0, 0].set_ylabel('Power (μV²/Hz)')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Band power across samples\n",
    "for band_name, powers in all_band_powers.items():\n",
    "    axes[0, 1].plot(powers, marker='o', label=band_name)\n",
    "axes[0, 1].set_title('Band Power Across Samples')\n",
    "axes[0, 1].set_xlabel('Sample Index')\n",
    "axes[0, 1].set_ylabel('Power (μV²/Hz)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Plot 3: Relative band power (normalized)\n",
    "normalized_powers = band_power_df.div(band_power_df.sum(axis=1), axis=0)\n",
    "normalized_powers.plot(kind='bar', stacked=True, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Relative Band Power (Normalized)')\n",
    "axes[1, 0].set_xlabel('Sample Index')\n",
    "axes[1, 0].set_ylabel('Relative Power')\n",
    "axes[1, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Plot 4: Power spectral density for one sample\n",
    "sample_eeg = eeg_data[0]  # First sample\n",
    "# Average across channels\n",
    "avg_signal = np.mean(sample_eeg, axis=1)\n",
    "freqs_psd, psd = signal.welch(avg_signal, fs=256, nperseg=64, noverlap=32, nfft=64)\n",
    "\n",
    "axes[1, 1].semilogy(freqs_psd, psd)\n",
    "axes[1, 1].set_title('Power Spectral Density (Sample 0, Averaged Across Channels)')\n",
    "axes[1, 1].set_xlabel('Frequency (Hz)')\n",
    "axes[1, 1].set_ylabel('Power Spectral Density (μV²/Hz)')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "# Add vertical lines for band boundaries\n",
    "band_boundaries = [1, 4, 8, 13, 30, 50]\n",
    "colors = ['red', 'orange', 'green', 'blue', 'purple']\n",
    "for i, freq in enumerate(band_boundaries[:-1]):\n",
    "    axes[1, 1].axvline(freq, color=colors[i], linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAnalyzed {n_samples_to_analyze} samples with {window_size}-sample windows\")\n",
    "print(\"Band frequency ranges:\")\n",
    "bands = {'delta': (1, 4), 'theta': (4, 8), 'alpha': (8, 13), 'beta': (13, 30), 'gamma': (30, 50)}\n",
    "for band_name, (low, high) in bands.items():\n",
    "    print(f\"  {band_name}: {low}-{high} Hz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create band power features for machine learning\n",
    "def extract_band_power_features(eeg_data, fs=256, window_size=64):\n",
    "    \"\"\"\n",
    "    Extract band power features from EEG data for machine learning.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    eeg_data : array-like, shape (n_samples, window_size, n_channels)\n",
    "        EEG data\n",
    "    fs : int, default=256\n",
    "        Sampling frequency in Hz\n",
    "    window_size : int, default=64\n",
    "        Window size for analysis\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    features : array, shape (n_samples, n_features)\n",
    "        Band power features for each sample\n",
    "    feature_names : list\n",
    "        Names of the features\n",
    "    \"\"\"\n",
    "    \n",
    "    n_samples, _, n_channels = eeg_data.shape\n",
    "    bands = ['delta', 'theta', 'alpha', 'beta', 'gamma']\n",
    "    \n",
    "    # Initialize feature matrix\n",
    "    # Features: [mean_power_per_band_per_channel, relative_power_per_band, total_power_per_channel]\n",
    "    n_features = len(bands) * n_channels + len(bands) + n_channels\n",
    "    features = np.zeros((n_samples, n_features))\n",
    "    \n",
    "    # Create feature names\n",
    "    feature_names = []\n",
    "    \n",
    "    # Band power per channel features\n",
    "    for band in bands:\n",
    "        for ch in range(n_channels):\n",
    "            feature_names.append(f'{band}_power_ch{ch:02d}')\n",
    "    \n",
    "    # Relative band power features (summed across channels)\n",
    "    for band in bands:\n",
    "        feature_names.append(f'{band}_relative_power')\n",
    "    \n",
    "    # Total power per channel\n",
    "    for ch in range(n_channels):\n",
    "        feature_names.append(f'total_power_ch{ch:02d}')\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        sample_eeg = eeg_data[i]  # Shape: (window_size, n_channels)\n",
    "        band_powers, _, _ = compute_band_power(sample_eeg, fs=fs, window_size=window_size)\n",
    "        \n",
    "        feature_idx = 0\n",
    "        \n",
    "        # Band power per channel\n",
    "        for band in bands:\n",
    "            powers = band_powers[band]  # Shape: (n_windows, n_channels)\n",
    "            mean_powers = np.mean(powers, axis=0)  # Average across windows\n",
    "            features[i, feature_idx:feature_idx+n_channels] = mean_powers\n",
    "            feature_idx += n_channels\n",
    "        \n",
    "        # Relative band power (summed across channels and normalized)\n",
    "        total_powers = []\n",
    "        for band in bands:\n",
    "            powers = band_powers[band]\n",
    "            total_power = np.sum(np.mean(powers, axis=0))  # Sum across channels, avg across windows\n",
    "            total_powers.append(total_power)\n",
    "        \n",
    "        # Normalize to get relative powers\n",
    "        total_sum = sum(total_powers)\n",
    "        if total_sum > 0:\n",
    "            relative_powers = [p / total_sum for p in total_powers]\n",
    "        else:\n",
    "            relative_powers = [0] * len(bands)\n",
    "        \n",
    "        features[i, feature_idx:feature_idx+len(bands)] = relative_powers\n",
    "        feature_idx += len(bands)\n",
    "        \n",
    "        # Total power per channel\n",
    "        channel_totals = np.zeros(n_channels)\n",
    "        for band in bands:\n",
    "            powers = band_powers[band]\n",
    "            channel_totals += np.mean(powers, axis=0)\n",
    "        \n",
    "        features[i, feature_idx:feature_idx+n_channels] = channel_totals\n",
    "    \n",
    "    return features, feature_names\n",
    "\n",
    "# Extract band power features for the entire dataset\n",
    "print(\"Extracting band power features for machine learning...\")\n",
    "print(f\"Processing {len(eeg_data)} samples...\")\n",
    "\n",
    "band_power_features, feature_names = extract_band_power_features(eeg_data, fs=256, window_size=64)\n",
    "\n",
    "print(f\"\\nExtracted features shape: {band_power_features.shape}\")\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "print(f\"Feature categories:\")\n",
    "print(f\"  - Band power per channel: {5 * 64} features\")\n",
    "print(f\"  - Relative band power: {5} features\") \n",
    "print(f\"  - Total power per channel: {64} features\")\n",
    "\n",
    "# Show some example features\n",
    "print(f\"\\nFirst 10 feature names:\")\n",
    "for i, name in enumerate(feature_names[:10]):\n",
    "    print(f\"  {i}: {name}\")\n",
    "\n",
    "print(f\"\\nLast 10 feature names:\")\n",
    "for i, name in enumerate(feature_names[-10:], len(feature_names)-10):\n",
    "    print(f\"  {i}: {name}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nFeature statistics:\")\n",
    "print(f\"  Mean: {np.mean(band_power_features):.6f}\")\n",
    "print(f\"  Std: {np.std(band_power_features):.6f}\")\n",
    "print(f\"  Min: {np.min(band_power_features):.6f}\")\n",
    "print(f\"  Max: {np.max(band_power_features):.6f}\")\n",
    "\n",
    "# Visualize feature distributions for first few features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(6):\n",
    "    axes[i].hist(band_power_features[:, i], bins=30, alpha=0.7)\n",
    "    axes[i].set_title(f'{feature_names[i]}')\n",
    "    axes[i].set_xlabel('Power')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBand power features extracted successfully!\")\n",
    "print(f\"These features can be used as input to machine learning models.\")\n",
    "print(f\"Shape: (n_samples={band_power_features.shape[0]}, n_features={band_power_features.shape[1]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "eeg_labels = np.array(y_resampled, dtype=int)\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    band_power_features, eeg_labels, test_size=0.2, random_state=42, stratify=eeg_labels\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"SVM (RBF kernel)\": SVC(kernel='rbf', random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Naive Bayes\": GaussianNB()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# You can adjust n_components for PCA as needed (e.g., 0.95 for 95% variance, or an int)\n",
    "pca_n_components = 0.95\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=pca_n_components, random_state=42)),\n",
    "        ('clf', model)\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results[name] = acc\n",
    "    print(f\"\\n{name} Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(list(results.items()), columns=[\"Model\", \"Accuracy\"])\n",
    "print(\"\\nSummary of model accuracies:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Braindecode datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from braindecode.datasets import create_from_X_y\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# define data root\n",
    "# this is the path to the ROAMM folder on local machine\n",
    "roamm_root = r\"/gpfs1/pi/anon/mindless_reading/ROAMM\"\n",
    "ml_data_root = os.path.join(roamm_root, 'subject_ml_data')\n",
    "# define subject id\n",
    "subject_id = 's10014'\n",
    "subject_dir = os.path.join(ml_data_root, subject_id)\n",
    "# load all runs for a subject\n",
    "pkl_files = [f for f in os.listdir(subject_dir) if f.endswith('.pkl')]\n",
    "df = pd.DataFrame()\n",
    "for pkl_file in pkl_files:\n",
    "    df_sub_single_run = pd.read_pickle(os.path.join(subject_dir, pkl_file))\n",
    "    df_sub_single_run = df_sub_single_run[df_sub_single_run['first_pass_reading'] == 1]\n",
    "    # convert bool col explicitly to avoid pandas warning\n",
    "    for col in ['is_blink', 'is_saccade', 'is_fixation', 'is_mw', 'first_pass_reading']:\n",
    "        df_sub_single_run[col] = df_sub_single_run[col] == True\n",
    "    df = pd.concat([df, df_sub_single_run])\n",
    "\n",
    "# normalize pupil size features\n",
    "df['blink_interp_LPupil_norm'] = df['blink_interp_LPupil'] / df['blink_interp_LPupil'].median()\n",
    "df['blink_interp_RPupil_norm'] = df['blink_interp_RPupil'] / df['blink_interp_RPupil'].median()\n",
    "\n",
    "# define EEG and eye tracking features\n",
    "ch_names = df.columns.tolist()[:64]  #first 64 columns are EEG channels\n",
    "sfreq = 256\n",
    "window_seconds = 0.5\n",
    "# Downsample data using 1-second windows (fs = 256 Hz)\n",
    "windowed_data = []\n",
    "windowed_labels = []\n",
    "window_size = int(sfreq * window_seconds)\n",
    "\n",
    "# Process data in chunks of window_size\n",
    "for i in range(0, len(df), window_size):\n",
    "    window = df.iloc[i:i+window_size]\n",
    "    # Skip if window is too small\n",
    "    if len(window) < window_size:\n",
    "        continue\n",
    "    # Check if labels are consistent in this window\n",
    "    labels_in_window = window['is_mw'].unique()\n",
    "    if len(labels_in_window) > 1:\n",
    "        # Skip windows with mixed labels\n",
    "        continue\n",
    "\n",
    "    # Extract features for this window: keep as 2D array (window_size x feature_number)\n",
    "    eeg_data = window[ch_names].values\n",
    "    windowed_data.append(eeg_data)\n",
    "    # Use the consistent label\n",
    "    windowed_labels.append(labels_in_window[0])\n",
    "\n",
    "# Use RandomUnderSampler on flattened data, then recover 3D structure\n",
    "windowed_data_flat = [w.flatten() for w in windowed_data]\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_resampled_flat, y_resampled = undersampler.fit_resample(windowed_data_flat, windowed_labels)\n",
    "# Recover 3D array: (n_samples, window_size, n_features)\n",
    "window_size = windowed_data[0].shape[0]\n",
    "n_features = windowed_data[0].shape[1]\n",
    "X_resampled = np.array(X_resampled_flat).reshape(-1, window_size, n_features)\n",
    "X = np.transpose(X_resampled, (0, 2, 1))  # (N, num_channels, window_size)\n",
    "y = np.array(y_resampled, dtype=int)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# create braindecode datasets for training and testing\n",
    "train_set = create_from_X_y(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    drop_last_window=False,\n",
    "    sfreq=sfreq,\n",
    "    ch_names=ch_names,\n",
    "    window_stride_samples=window_size,\n",
    "    window_size_samples=window_size\n",
    ")\n",
    "\n",
    "test_set = create_from_X_y(\n",
    "    X_test,\n",
    "    y_test,\n",
    "    drop_last_window=False,\n",
    "    sfreq=sfreq,\n",
    "    ch_names=ch_names,\n",
    "    window_stride_samples=window_size,\n",
    "    window_size_samples=window_size\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codecs import raw_unicode_escape_decode\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from skorch.callbacks import EarlyStopping, EpochScoring\n",
    "from skorch.dataset import ValidSplit\n",
    "from braindecode import EEGClassifier\n",
    "from braindecode.models import EEGNet, ATCNet\n",
    "from braindecode.util import set_random_seeds\n",
    "from skorch.callbacks import LRScheduler\n",
    "from braindecode.datasets import BaseConcatDataset, BaseDataset\n",
    "from braindecode.preprocessing import create_fixed_length_windows\n",
    "\n",
    "# Set up GPU if it is there\n",
    "cuda = torch.cuda.is_available() \n",
    "device = \"cuda\" if cuda else \"cpu\"\n",
    "\n",
    "# Set random seed to be able to reproduce results\n",
    "seed = 42\n",
    "set_random_seeds(seed=seed, cuda=cuda)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# define model\n",
    "model = EEGNet(n_chans=64, n_outputs=2, n_times=window_size)\n",
    "# model = ATCNet(n_chans=64, n_outputs=2, input_window_seconds=window_seconds, sfreq=sfreq, chs_info=ch_names, n_times=window_size)\n",
    "# print(model)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "# define training hyperparameters\n",
    "lr = 0.0625 * 0.1\n",
    "weight_decay = 0\n",
    "batch_size = 32\n",
    "n_epochs = 100\n",
    "patience = 3\n",
    "\n",
    "clf = EEGClassifier(\n",
    "    model,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    train_split=None,\n",
    "    optimizer__lr=lr,\n",
    "    optimizer__weight_decay=weight_decay,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[\n",
    "        \"accuracy\",\n",
    "        (\"lr_scheduler\", LRScheduler(\"CosineAnnealingLR\", T_max=n_epochs - 1)),\n",
    "    ],\n",
    "    verbose=1,  # Not printing the results for each epoch\n",
    "    device=device,\n",
    "    classes=list(range(2)),\n",
    "    max_epochs=n_epochs,\n",
    ")\n",
    "\n",
    "# Model training for a specified number of epochs. `y` is None as it is already supplied\n",
    "# in the dataset.\n",
    "clf.fit(train_set, y=None)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict labels\n",
    "y_pred = clf.predict(test_set)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "disp.plot(ax=ax, cmap='Blues', colorbar=False)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# Print additional metrics\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Weighted F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mne\n",
    "\n",
    "# define data root\n",
    "# this is the path to the ROAMM folder on local machine\n",
    "roamm_root = r\"/gpfs1/pi/anon/mindless_reading/ROAMM\"\n",
    "ml_data_root = os.path.join(roamm_root, 'subject_ml_data')\n",
    "# define subject id\n",
    "subject_id = 's10014'\n",
    "subject_dir = os.path.join(ml_data_root, subject_id, 'window_datasets')\n",
    "# load single data\n",
    "data = np.load(os.path.join(subject_dir, 's10014_256windowed_data.npy'))\n",
    "labels = np.load(os.path.join(subject_dir, 's10014_256windowed_labels.npy'))\n",
    "col_name = np.load(os.path.join(subject_dir, 's10014_col_names.npy'))\n",
    "sfreq = 256\n",
    "\n",
    "# get the 64-channel eeg data (first 64 cols)\n",
    "eeg_data = data[:, :64, :]\n",
    "eeg_data = eeg_data.astype(np.float64)\n",
    "psds, freqs = mne.time_frequency.psd_array_multitaper(eeg_data, sfreq, fmax=50, remove_dc=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot each channel PSD for on-task and mind-wandering\n",
    "labels = labels.astype(bool)\n",
    "psds_ot = np.mean(psds[~labels], axis=0)\n",
    "psds_mw = np.mean(psds[labels], axis=0)\n",
    "\n",
    "# Convert to dB (decibels)\n",
    "# psds_ot_db = 10 * np.log10(psds_ot)\n",
    "# psds_mw_db = 10 * np.log10(psds_mw)\n",
    "\n",
    "print(\"Creating individual channel plots...\")\n",
    "\n",
    "# Create the plot with proper indexing\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Plot on-task condition (all 64 channels)\n",
    "plt.subplot(2, 1, 1)\n",
    "# for ch_idx in range(psds_ot_db.shape[0]):  # Iterate over channels\n",
    "#     plt.plot(freqs, psds_ot_db[ch_idx, :], alpha=0.7, linewidth=0.8)\n",
    "\n",
    "for ch_idx in range(psds_ot.shape[0]):  # Iterate over channels\n",
    "    plt.plot(freqs, psds_ot[ch_idx, :], alpha=0.7, linewidth=0.8)\n",
    "\n",
    "plt.title('On-Task: Power Spectral Density for All 64 EEG Channels', fontsize=14)\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Power (dB)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 50)\n",
    "\n",
    "# Plot mind-wandering condition (all 64 channels)\n",
    "plt.subplot(2, 1, 2)\n",
    "# for ch_idx in range(psds_mw_db.shape[0]):  # Iterate over channels\n",
    "#     plt.plot(freqs, psds_mw_db[ch_idx, :], alpha=0.7, linewidth=0.8)\n",
    "\n",
    "for ch_idx in range(psds_mw.shape[0]):  # Iterate over channels\n",
    "    plt.plot(freqs, psds_mw[ch_idx, :], alpha=0.7, linewidth=0.8)\n",
    "\n",
    "plt.title('Mind-Wandering: Power Spectral Density for All 64 EEG Channels', fontsize=14)\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "# plt.ylabel('Power (dB)')\n",
    "plt.ylabel('Power (μV²/Hz)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 50)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"SVM (RBF kernel)\": SVC(kernel='rbf', random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Naive Bayes\": GaussianNB()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# You can adjust n_components for PCA as needed (e.g., 0.95 for 95% variance, or an int)\n",
    "pca_n_components = 0.95\n",
    "\n",
    "# Split the data\n",
    "psds_flattened = psds.reshape(psds.shape[0], -1)\n",
    "# convert to dB\n",
    "psds_flattened = 10 * np.log10(psds_flattened)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    psds_flattened, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=pca_n_components, random_state=42)),\n",
    "        ('clf', model)\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results[name] = acc\n",
    "    print(f\"\\n{name} Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(list(results.items()), columns=[\"Model\", \"Accuracy\"])\n",
    "print(\"\\nSummary of model accuracies:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate feature files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ==========================\n",
    "# Config\n",
    "# ==========================\n",
    "data_root = \"/gpfs1/pi/anon/mindless_reading/data\"\n",
    "sfreq = 256\n",
    "window_seconds = 2\n",
    "window_size = int(sfreq * window_seconds)\n",
    "\n",
    "# ==========================\n",
    "# EEG band definitions (global)\n",
    "# ==========================\n",
    "band_names = [\n",
    "    \"theta1\", \"theta2\", \"alpha1\", \"alpha2\",\n",
    "    \"beta1\", \"beta2\", \"gamma1\", \"gamma2\",\n",
    "]\n",
    "\n",
    "band_defs = [\n",
    "    (4.0, 6.0),    # theta1\n",
    "    (6.5, 8.0),    # theta2\n",
    "    (8.5, 10.0),   # alpha1\n",
    "    (10.5, 13.0),  # alpha2\n",
    "    (13.5, 18.0),  # beta1\n",
    "    (18.5, 30.0),  # beta2\n",
    "    (30.5, 40.0),  # gamma1\n",
    "    (40.0, 49.5),  # gamma2\n",
    "]\n",
    "\n",
    "n_bands = len(band_defs)\n",
    "\n",
    "# ==========================\n",
    "# Helper functions\n",
    "# ==========================\n",
    "def get_col_array(data, col_names, targets):\n",
    "    \"\"\"\n",
    "    Return array shaped (n_epochs, seq_len) for the first matching column\n",
    "    in `targets`. If column not found, return None.\n",
    "    \"\"\"\n",
    "    idx = np.where(np.isin(col_names, targets))[0]\n",
    "    if idx.size == 0:\n",
    "        return None\n",
    "\n",
    "    arr = data[:, idx].astype(np.float64)  # (n_epochs, 1, seq_len) typically\n",
    "    if arr.ndim == 3 and arr.shape[1] == 1:\n",
    "        arr = arr[:, 0, :]  # (n_epochs, seq_len)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def mean_per_epoch(arr):\n",
    "    \"\"\"\n",
    "    Compute per-epoch mean across valid (non-NaN) timepoints.\n",
    "    Returns list of length n_epochs; NaN if no valid points.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    mask = ~np.isnan(arr)\n",
    "    for row, m in zip(arr, mask):\n",
    "        if m.any():\n",
    "            out.append(np.nanmean(row[m]))\n",
    "        else:\n",
    "            out.append(np.nan)\n",
    "    return out\n",
    "\n",
    "\n",
    "def unique_counts_per_epoch(arr):\n",
    "    \"\"\"\n",
    "    Count unique valid values per epoch (0 if none).\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    mask = ~np.isnan(arr)\n",
    "    for row, m in zip(arr, mask):\n",
    "        if m.any():\n",
    "            out.append(np.unique(row[m]).size)\n",
    "        else:\n",
    "            out.append(0)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Collect subjects\n",
    "# ==========================\n",
    "all_subjects = sorted(\n",
    "    d for d in os.listdir(data_root)\n",
    "    if d.startswith(\"s\") and os.path.isdir(os.path.join(data_root, d))\n",
    ")\n",
    "\n",
    "epoch_counts = []\n",
    "\n",
    "# ==========================\n",
    "# Main loop over subjects\n",
    "# ==========================\n",
    "for subject_id in all_subjects:\n",
    "    subject_dir = os.path.join(\n",
    "        data_root,\n",
    "        subject_id,\n",
    "        \"ml_data\",\n",
    "        f\"{window_size}window_datasets\",\n",
    "    )\n",
    "\n",
    "    data_file = os.path.join(subject_dir, f\"{subject_id}_{window_size}windowed_data.npy\")\n",
    "    label_file = os.path.join(subject_dir, f\"{subject_id}_{window_size}windowed_labels.npy\")\n",
    "    col_file = os.path.join(subject_dir, f\"{subject_id}_col_names.npy\")\n",
    "\n",
    "    # Skip subjects with missing files\n",
    "    if not (os.path.exists(data_file) and os.path.exists(label_file) and os.path.exists(col_file)):\n",
    "        print(f\"Missing files for {subject_id}, skipping.\")\n",
    "        epoch_counts.append(0)\n",
    "        continue\n",
    "\n",
    "    data = np.load(data_file)\n",
    "    labels = np.load(label_file)\n",
    "    col_name = np.load(col_file)\n",
    "\n",
    "    epoch_count = data.shape[0]\n",
    "    epoch_counts.append(epoch_count)\n",
    "\n",
    "    # check sample count for every subject\n",
    "    if epoch_count < 10:\n",
    "        print(f\"Subject {subject_id} has insufficient data samples: {epoch_count} samples.\")\n",
    "        continue\n",
    "\n",
    "    # ==========================\n",
    "    # EEG PSD features\n",
    "    # ==========================\n",
    "    # get the 64-channel EEG data (first 64 cols)\n",
    "    eeg_data = data[:, :64, :].astype(np.float64)\n",
    "\n",
    "    psds, freqs = mne.time_frequency.psd_array_multitaper(\n",
    "        eeg_data,\n",
    "        sfreq,\n",
    "        fmin=4,\n",
    "        fmax=50,\n",
    "        output=\"power\",\n",
    "    )\n",
    "\n",
    "    n_epochs, n_ch, _ = psds.shape\n",
    "\n",
    "    psds_band = np.empty((n_epochs, n_ch, n_bands), dtype=psds.dtype)\n",
    "    for i, (fmin, fmax) in enumerate(band_defs):\n",
    "        idx = (freqs >= fmin) & (freqs <= fmax)\n",
    "        psds_band[:, :, i] = psds[:, :, idx].mean(axis=-1)\n",
    "\n",
    "    # Flatten features for ML: (n_epochs, n_ch * n_bands)\n",
    "    psds_band_flat = psds_band.reshape(n_epochs, -1)\n",
    "\n",
    "    # column names: chan_band\n",
    "    eeg_columns = [\n",
    "        f\"{ch}_{band}\"\n",
    "        for ch in col_name[:n_ch]\n",
    "        for band in band_names\n",
    "    ]\n",
    "    df_eeg = pd.DataFrame(psds_band_flat, columns=eeg_columns)\n",
    "\n",
    "    # ==========================\n",
    "    # Eye-tracking features\n",
    "    # ==========================\n",
    "    df_eye = pd.DataFrame(index=np.arange(epoch_count))\n",
    "\n",
    "    # fixation count\n",
    "    arr = get_col_array(data, col_name, [\"fix_R_tStart\"])\n",
    "    if arr is not None:\n",
    "        df_eye[\"fix_count\"] = unique_counts_per_epoch(arr)\n",
    "\n",
    "    # average fixation duration\n",
    "    arr = get_col_array(data, col_name, [\"fix_R_duration\"])\n",
    "    if arr is not None:\n",
    "        df_eye[\"fix_R_duration\"] = mean_per_epoch(arr)\n",
    "\n",
    "    # fixation pupil average\n",
    "    arr = get_col_array(data, col_name, [\"fix_R_pupilAvg\"])\n",
    "    if arr is not None:\n",
    "        df_eye[\"fix_R_pupilAvg\"] = mean_per_epoch(arr)\n",
    "\n",
    "    # saccade count\n",
    "    arr = get_col_array(data, col_name, [\"sacc_R_tStart\"])\n",
    "    if arr is not None:\n",
    "        df_eye[\"sacc_count\"] = unique_counts_per_epoch(arr)\n",
    "\n",
    "    # saccade duration\n",
    "    arr = get_col_array(data, col_name, [\"sacc_R_duration\"])\n",
    "    if arr is not None:\n",
    "        df_eye[\"sacc_R_duration\"] = mean_per_epoch(arr)\n",
    "\n",
    "    # saccade amplitude\n",
    "    arr = get_col_array(data, col_name, [\"sacc_R_ampDeg\"])\n",
    "    if arr is not None:\n",
    "        df_eye[\"sacc_R_ampDeg\"] = mean_per_epoch(arr)\n",
    "\n",
    "    # saccade peak velocity\n",
    "    arr = get_col_array(data, col_name, [\"sacc_R_vPeak\"])\n",
    "    if arr is not None:\n",
    "        df_eye[\"sacc_R_vPeak\"] = mean_per_epoch(arr)\n",
    "\n",
    "    # pupil (normalized)\n",
    "    arr = get_col_array(data, col_name, [\"blink_interp_RPupil_norm\"])\n",
    "    if arr is not None:\n",
    "        df_eye[\"pupil_avg\"] = mean_per_epoch(arr)\n",
    "\n",
    "    # drop all-NaN columns if any\n",
    "    df_eye = df_eye.dropna(axis=1, how=\"all\")\n",
    "\n",
    "    # ==========================\n",
    "    # Combine EEG + eye features and save\n",
    "    # ==========================\n",
    "    df_data = pd.concat([df_eeg, df_eye], axis=1)\n",
    "    df_data[\"label\"] = labels\n",
    "    df_data[\"subject_id\"] = subject_id\n",
    "\n",
    "    out_file = os.path.join(\n",
    "        subject_dir,\n",
    "        f\"{subject_id}_{window_size}windowed_features.csv\",\n",
    "    )\n",
    "    df_data.to_csv(out_file, index=False)\n",
    "    print(f\"Saved features for {subject_id} to: {out_file}\")\n",
    "\n",
    "# ==========================\n",
    "# Save epoch counts summary\n",
    "# ==========================\n",
    "df_epoch_counts = pd.DataFrame({\n",
    "    \"subject_id\": all_subjects,\n",
    "    \"n_epochs\": epoch_counts,\n",
    "})\n",
    "summary_file = os.path.join(\n",
    "    data_root,\n",
    "    f\"all_subjects_{window_size}_window_epoch_counts.csv\",\n",
    ")\n",
    "df_epoch_counts.to_csv(summary_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_root = \"/gpfs1/pi/anon/mindless_reading/data\"\n",
    "sfreq = 256\n",
    "window_seconds = 2\n",
    "window_size = int(sfreq * window_seconds)\n",
    "\n",
    "# collect subjects\n",
    "all_subjects = sorted(\n",
    "    d for d in os.listdir(data_root)\n",
    "    if d.startswith(\"s\") and os.path.isdir(os.path.join(data_root, d))\n",
    ")\n",
    "\n",
    "dfs = []   # store subject dfs here\n",
    "\n",
    "for subject_id in all_subjects:\n",
    "    subject_dir = os.path.join(\n",
    "        data_root,\n",
    "        subject_id,\n",
    "        \"ml_data\",\n",
    "        f\"{window_size}window_datasets\",\n",
    "    )\n",
    "\n",
    "    feature_file = os.path.join(\n",
    "        subject_dir,\n",
    "        f\"{subject_id}_{window_size}windowed_features.csv\",\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(feature_file):\n",
    "        print(f\"Missing features for {subject_id}, skipping\")\n",
    "        continue\n",
    "\n",
    "    df_sub = pd.read_csv(feature_file)\n",
    "\n",
    "    # safety check\n",
    "    if \"subject_id\" not in df_sub.columns:\n",
    "        df_sub[\"subject_id\"] = subject_id\n",
    "\n",
    "    dfs.append(df_sub)\n",
    "\n",
    "# concatenate once all subjects are processed\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "# save combined dataframe\n",
    "df.to_csv(os.path.join(data_root, f\"all_subjects_{window_size}windowed_features.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group MW classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    ")\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# ==========================\n",
    "# Config & data loading\n",
    "# ==========================\n",
    "data_root = \"/gpfs1/pi/anon/mindless_reading/data\"\n",
    "sfreq = 256\n",
    "window_seconds = 2\n",
    "window_size = int(sfreq * window_seconds)\n",
    "\n",
    "df = pd.read_csv(\n",
    "    os.path.join(data_root, f\"all_subjects_{window_size}windowed_features.csv\")\n",
    ")\n",
    "df = df.dropna(axis=0, how=\"any\").reset_index(drop=True)\n",
    "\n",
    "pca_variance = 0.95  # keep 95% variance; or set an int for n_components\n",
    "\n",
    "# Features: everything except label and subject_id\n",
    "feature_cols = [c for c in df.columns if c not in [\"label\", \"subject_id\"]]\n",
    "X = df[feature_cols].values\n",
    "y = df[\"label\"].values\n",
    "groups = df[\"subject_id\"].values  # for LOSO\n",
    "\n",
    "# Separate EEG and eye-tracking features\n",
    "# fix_count is the first eye-feature column\n",
    "idx_fix = np.where(df.columns.str.contains(\"fix_count\"))[0][0]\n",
    "\n",
    "X_eeg = X[:, :idx_fix]   # EEG-only features\n",
    "X_eye = X[:, idx_fix:]   # Eye-only features\n",
    "\n",
    "# ==========================\n",
    "# Models\n",
    "# ==========================\n",
    "base_models = {\n",
    "    \"logreg\": LogisticRegression(max_iter=1000, n_jobs=-1),\n",
    "    \"linear_svc\": LinearSVC(),  # no probas, but we can use decision_function\n",
    "    \"rbf_svc\": SVC(kernel=\"rbf\", probability=True),\n",
    "    \"random_forest\": RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=None,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "    ),\n",
    "    \"gradient_boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"knn\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"mlp\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
    "}\n",
    "\n",
    "# ==========================\n",
    "# LOSO + prediction logging\n",
    "# ==========================\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "all_preds = []  # per-sample predictions across all folds / models / feature sets\n",
    "\n",
    "for feature_set, X_data in zip(\n",
    "    [\"EEG + Eye\", \"EEG\", \"Eye\"],\n",
    "    [X, X_eeg, X_eye],\n",
    "):\n",
    "    print(f\"\\n========== Feature set: {feature_set} ==========\")\n",
    "\n",
    "    for model_name, base_clf in base_models.items():\n",
    "        print(f\"\\n=== Model: {model_name} ===\")\n",
    "\n",
    "        acc_list = []\n",
    "        f1_list = []\n",
    "        prec_list = []\n",
    "        auc_list = []\n",
    "\n",
    "        for train_idx, test_idx in logo.split(X_data, y, groups=groups):\n",
    "            subj_test = np.unique(groups[test_idx])\n",
    "            assert len(subj_test) == 1  # LOSO: only one subject held out\n",
    "            subj_test = subj_test[0]\n",
    "\n",
    "            X_train, X_test = X_data[train_idx], X_data[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            # fresh clone of classifier\n",
    "            clf = clone(base_clf)\n",
    "\n",
    "            # Pipeline: StandardScaler -> PCA -> classifier\n",
    "            pipe = Pipeline([\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"pca\", PCA(n_components=pca_variance)),\n",
    "                (\"clf\", clf),\n",
    "            ])\n",
    "\n",
    "            pipe.fit(X_train, y_train)\n",
    "            y_pred = pipe.predict(X_test)\n",
    "\n",
    "            # scores for AUC\n",
    "            y_scores = None\n",
    "            if hasattr(pipe, \"predict_proba\"):\n",
    "                try:\n",
    "                    y_scores = pipe.predict_proba(X_test)[:, 1]\n",
    "                except Exception:\n",
    "                    y_scores = None\n",
    "            elif hasattr(pipe, \"decision_function\"):\n",
    "                try:\n",
    "                    y_scores = pipe.decision_function(X_test)\n",
    "                except Exception:\n",
    "                    y_scores = None\n",
    "\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "            if (y_scores is not None) and (np.unique(y_test).size == 2):\n",
    "                auc = roc_auc_score(y_test, y_scores)\n",
    "            else:\n",
    "                auc = np.nan\n",
    "\n",
    "            acc_list.append(acc)\n",
    "            f1_list.append(f1)\n",
    "            prec_list.append(prec)\n",
    "            auc_list.append(auc)\n",
    "\n",
    "            # log per-subject fold summary\n",
    "            print(\n",
    "                f\"  Subject {subj_test}: \"\n",
    "                f\"acc={acc:.3f}, prec={prec:.3f}, f1={f1:.3f}, \"\n",
    "                f\"auc={auc if not np.isnan(auc) else float('nan'):.3f}\"\n",
    "            )\n",
    "\n",
    "            # store per-sample predictions for this fold\n",
    "            for i, idx in enumerate(test_idx):\n",
    "                all_preds.append({\n",
    "                    \"feature_set\": feature_set,\n",
    "                    \"model\": model_name,\n",
    "                    \"test_subject\": subj_test,\n",
    "                    \"sample_idx\": int(idx),\n",
    "                    \"y_true\": int(y_test[i]),\n",
    "                    \"y_pred\": int(y_pred[i]),\n",
    "                    \"y_score\": float(y_scores[i]) if y_scores is not None else np.nan,\n",
    "                })\n",
    "            \n",
    "            # break  # TEMP: only do one fold for testing; REMOVE for full LOSO\n",
    "\n",
    "        # per-model, per-feature-set mean over subjects\n",
    "        print(\n",
    "            f\"Mean over subjects — acc={np.mean(acc_list):.3f}, \"\n",
    "            f\"prec={np.mean(prec_list):.3f}, \"\n",
    "            f\"f1={np.mean(f1_list):.3f}, \"\n",
    "            f\"auc={np.nanmean(auc_list):.3f}\"\n",
    "        )\n",
    "\n",
    "# ==========================\n",
    "# Build DataFrame of predictions\n",
    "# ==========================\n",
    "df_preds = pd.DataFrame(all_preds)\n",
    "\n",
    "results_dir = os.path.join(data_root, \"ml_results\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "pred_file = os.path.join(\n",
    "    results_dir,\n",
    "    f\"loso_predictions_{window_size}win.csv\",\n",
    ")\n",
    "df_preds.to_csv(pred_file, index=False)\n",
    "print(f\"\\nSaved per-sample LOSO predictions to {pred_file}\")\n",
    "\n",
    "# ==========================\n",
    "# Final metrics from saved predictions (overall)\n",
    "# ==========================\n",
    "rows = []\n",
    "for (feature_set, model_name), g in df_preds.groupby([\"feature_set\", \"model\"]):\n",
    "    y_true = g[\"y_true\"].values\n",
    "    y_pred = g[\"y_pred\"].values\n",
    "    y_score = g[\"y_score\"].values\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    # AUC over all samples for this (feature_set, model)\n",
    "    if np.unique(y_true).size == 2 and not np.all(np.isnan(y_score)):\n",
    "        auc = roc_auc_score(y_true, y_score)\n",
    "    else:\n",
    "        auc = np.nan\n",
    "\n",
    "    rows.append({\n",
    "        \"feature_set\": feature_set,\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "        \"n_samples\": len(g),\n",
    "    })\n",
    "\n",
    "df_metrics = pd.DataFrame(rows)\n",
    "\n",
    "metrics_file = os.path.join(\n",
    "    results_dir,\n",
    "    f\"loso_metrics_{window_size}win.csv\",\n",
    ")\n",
    "df_metrics.to_csv(metrics_file, index=False)\n",
    "\n",
    "print(f\"\\nSaved aggregated metrics to {metrics_file}\")\n",
    "print(\"\\n=== Overall summary ===\")\n",
    "print(df_metrics.sort_values([\"feature_set\", \"accuracy\"], ascending=[True, False]))\n",
    "\n",
    "# ==========================\n",
    "# Subject-level metrics\n",
    "# ==========================\n",
    "rows_subj = []\n",
    "for (feature_set, model_name, subj), g in df_preds.groupby(\n",
    "    [\"feature_set\", \"model\", \"test_subject\"]\n",
    "):\n",
    "    y_true = g[\"y_true\"].values\n",
    "    y_pred = g[\"y_pred\"].values\n",
    "    y_score = g[\"y_score\"].values\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    if np.unique(y_true).size == 2 and not np.all(np.isnan(y_score)):\n",
    "        auc = roc_auc_score(y_true, y_score)\n",
    "    else:\n",
    "        auc = np.nan\n",
    "\n",
    "    rows_subj.append({\n",
    "        \"feature_set\": feature_set,\n",
    "        \"model\": model_name,\n",
    "        \"subject_id\": subj,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "        \"n_samples\": len(g),\n",
    "    })\n",
    "\n",
    "df_subject_metrics = pd.DataFrame(rows_subj)\n",
    "\n",
    "subj_metrics_file = os.path.join(\n",
    "    results_dir,\n",
    "    f\"loso_subject_metrics_{window_size}win.csv\",\n",
    ")\n",
    "df_subject_metrics.to_csv(subj_metrics_file, index=False)\n",
    "\n",
    "print(f\"\\nSaved subject-level metrics to {subj_metrics_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    ")\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# ==========================\n",
    "# Config & data loading\n",
    "# ==========================\n",
    "data_root = \"/gpfs1/pi/anon/mindless_reading/data\"\n",
    "sfreq = 256\n",
    "window_seconds = 2\n",
    "window_size = int(sfreq * window_seconds)\n",
    "\n",
    "df = pd.read_csv(\n",
    "    os.path.join(data_root, f\"all_subjects_{window_size}windowed_features.csv\")\n",
    ")\n",
    "df = df.dropna(axis=0, how=\"any\").reset_index(drop=True)\n",
    "\n",
    "pca_variance = 0.95  # keep 95% variance; or set an int for n_components\n",
    "\n",
    "# Features: everything except label and subject_id\n",
    "feature_cols = [c for c in df.columns if c not in [\"label\", \"subject_id\"]]\n",
    "X = df[feature_cols].values\n",
    "y = df[\"label\"].values\n",
    "groups = df[\"subject_id\"].values  # for LOSO\n",
    "\n",
    "# Train the random forest classifier on all data for feature importance\n",
    "# (This is just for feature importance visualization, not for evaluation)\n",
    "X_train_scaled = StandardScaler().fit_transform(X)\n",
    "pca = PCA(n_components=pca_variance)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "rf_clf.fit(X_train_pca, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf_clf.feature_importances_\n",
    "# Sort indices by importance\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Get the PCA component names\n",
    "n_components = X_train_pca.shape[1]\n",
    "pca_feature_names = [f\"PC{i+1}\" for i in range(n_components)]\n",
    "\n",
    "# Plot feature importance for top 20 components\n",
    "n_features_to_plot = min(20, n_components)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importance - Random Forest (Top 20 PCA Components)\", fontsize=14, fontweight='bold')\n",
    "plt.bar(range(n_features_to_plot), importances[indices[:n_features_to_plot]], align='center')\n",
    "plt.xticks(range(n_features_to_plot), [pca_feature_names[i] for i in indices[:n_features_to_plot]], rotation=45, ha='right')\n",
    "plt.xlabel(\"PCA Components\", fontsize=12)\n",
    "plt.ylabel(\"Importance\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Also print the original feature contributions (by looking at PCA loadings)\n",
    "print(\"\\n=== Top Contributing Original Features to Most Important PCA Components ===\")\n",
    "for pc_idx in indices[:5]:\n",
    "    print(f\"\\n{pca_feature_names[pc_idx]} (Importance: {importances[pc_idx]:.4f})\")\n",
    "    loadings = np.abs(pca.components_[pc_idx])\n",
    "    top_feat_idx = np.argsort(loadings)[::-1][:5]\n",
    "    for feat_idx in top_feat_idx:\n",
    "        print(f\"  {feature_cols[feat_idx]}: {loadings[feat_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ==========================\n",
    "# Config & data loading\n",
    "# ==========================\n",
    "data_root = \"/gpfs1/pi/anon/mindless_reading/data\"\n",
    "sfreq = 256\n",
    "window_seconds = 2\n",
    "window_size = int(sfreq * window_seconds)\n",
    "\n",
    "df = pd.read_csv(\n",
    "    os.path.join(data_root, f\"all_subjects_{window_size}windowed_features.csv\")\n",
    ")\n",
    "df = df.dropna(axis=0, how=\"any\").reset_index(drop=True)\n",
    "\n",
    "pca_variance = 0.95  # keep 95% variance\n",
    "\n",
    "# ==========================\n",
    "# Features & groups\n",
    "# ==========================\n",
    "feature_cols = [c for c in df.columns if c not in [\"label\", \"subject_id\"]]\n",
    "X = df[feature_cols].values\n",
    "y = df[\"label\"].values\n",
    "groups = df[\"subject_id\"].values  # for LOSO\n",
    "\n",
    "# ==========================\n",
    "# PERMUTE LABELS (sanity test)\n",
    "# ==========================\n",
    "rng = np.random.default_rng(42)\n",
    "y = rng.permutation(y)\n",
    "\n",
    "# ==========================\n",
    "# Separate EEG & Eye features\n",
    "# ==========================\n",
    "idx_fix = np.where(df.columns.str.contains(\"fix_count\"))[0][0]\n",
    "\n",
    "X_eeg = X[:, :idx_fix]\n",
    "X_eye = X[:, idx_fix:]\n",
    "\n",
    "# ==========================\n",
    "# Random Forest Model\n",
    "# ==========================\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# LOSO CV + logging\n",
    "# ==========================\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "all_preds = []\n",
    "\n",
    "for feature_set, X_data in zip(\n",
    "    [\"EEG + Eye\", \"EEG\", \"Eye\"],\n",
    "    [X, X_eeg, X_eye],\n",
    "):\n",
    "    print(f\"\\n========== Feature set: {feature_set} (PERMUTED LABELS) ==========\")\n",
    "\n",
    "    acc_list = []\n",
    "    f1_list = []\n",
    "    prec_list = []\n",
    "    auc_list = []\n",
    "\n",
    "    for train_idx, test_idx in logo.split(X_data, y, groups=groups):\n",
    "        subj_test = np.unique(groups[test_idx])[0]\n",
    "\n",
    "        X_train, X_test = X_data[train_idx], X_data[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"pca\", PCA(n_components=pca_variance)),\n",
    "            (\"clf\", rf_clf),\n",
    "        ])\n",
    "\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_pred = pipe.predict(X_test)\n",
    "\n",
    "        # Probabilities → for AUC\n",
    "        try:\n",
    "            y_scores = pipe.predict_proba(X_test)[:, 1]\n",
    "        except Exception:\n",
    "            y_scores = None\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "        auc = (\n",
    "            roc_auc_score(y_test, y_scores)\n",
    "            if (y_scores is not None and len(np.unique(y_test)) == 2)\n",
    "            else np.nan\n",
    "        )\n",
    "\n",
    "        acc_list.append(acc)\n",
    "        f1_list.append(f1)\n",
    "        prec_list.append(prec)\n",
    "        auc_list.append(auc)\n",
    "\n",
    "        print(\n",
    "            f\"  Subject {subj_test}: \"\n",
    "            f\"acc={acc:.3f}, prec={prec:.3f}, f1={f1:.3f}, \"\n",
    "            f\"auc={auc if not np.isnan(auc) else float('nan'):.3f}\"\n",
    "        )\n",
    "\n",
    "        # store sample-level predictions\n",
    "        for i, idx in enumerate(test_idx):\n",
    "            all_preds.append({\n",
    "                \"feature_set\": feature_set,\n",
    "                \"model\": \"random_forest\",\n",
    "                \"test_subject\": subj_test,\n",
    "                \"sample_idx\": int(idx),\n",
    "                \"y_true\": int(y_test[i]),\n",
    "                \"y_pred\": int(y_pred[i]),\n",
    "                \"y_score\": float(y_scores[i]) if y_scores is not None else np.nan,\n",
    "            })\n",
    "\n",
    "    print(\n",
    "        f\"Mean over subjects — acc={np.mean(acc_list):.3f}, \"\n",
    "        f\"prec={np.mean(prec_list):.3f}, \"\n",
    "        f\"f1={np.mean(f1_list):.3f}, \"\n",
    "        f\"auc={np.nanmean(auc_list):.3f}\"\n",
    "    )\n",
    "\n",
    "# ==========================\n",
    "# Save predictions\n",
    "# ==========================\n",
    "df_preds = pd.DataFrame(all_preds)\n",
    "\n",
    "results_dir = os.path.join(data_root, \"ml_results\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "pred_file = os.path.join(\n",
    "    results_dir,\n",
    "    f\"loso_predictions_perm_RF_{window_size}win.csv\",\n",
    ")\n",
    "df_preds.to_csv(pred_file, index=False)\n",
    "print(f\"\\nSaved permuted RF predictions to {pred_file}\")\n",
    "\n",
    "# ==========================\n",
    "# Overall metrics\n",
    "# ==========================\n",
    "rows = []\n",
    "for feature_set, g in df_preds.groupby(\"feature_set\"):\n",
    "    y_true = g[\"y_true\"].values\n",
    "    y_pred = g[\"y_pred\"].values\n",
    "    y_score = g[\"y_score\"].values\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    auc = (\n",
    "        roc_auc_score(y_true, y_score)\n",
    "        if (np.unique(y_true).size == 2 and not np.all(np.isnan(y_score)))\n",
    "        else np.nan\n",
    "    )\n",
    "\n",
    "    rows.append({\n",
    "        \"feature_set\": feature_set,\n",
    "        \"model\": \"random_forest\",\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "        \"n_samples\": len(g),\n",
    "    })\n",
    "\n",
    "df_metrics = pd.DataFrame(rows)\n",
    "\n",
    "metrics_file = os.path.join(\n",
    "    results_dir,\n",
    "    f\"loso_metrics_perm_RF_{window_size}win.csv\",\n",
    ")\n",
    "df_metrics.to_csv(metrics_file, index=False)\n",
    "\n",
    "print(f\"\\nSaved permuted RF metrics to {metrics_file}\")\n",
    "print(\"\\n=== Summary (Permuted Labels — RF Only) ===\")\n",
    "print(df_metrics)\n",
    "\n",
    "# ==========================\n",
    "# Subject-level metrics\n",
    "# ==========================\n",
    "rows_subj = []\n",
    "for (feature_set, subj), g in df_preds.groupby([\"feature_set\", \"test_subject\"]):\n",
    "    y_true = g[\"y_true\"].values\n",
    "    y_pred = g[\"y_pred\"].values\n",
    "    y_score = g[\"y_score\"].values\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    auc = (\n",
    "        roc_auc_score(y_true, y_score)\n",
    "        if (np.unique(y_true).size == 2 and not np.all(np.isnan(y_score)))\n",
    "        else np.nan\n",
    "    )\n",
    "\n",
    "    rows_subj.append({\n",
    "        \"feature_set\": feature_set,\n",
    "        \"model\": \"random_forest\",\n",
    "        \"subject_id\": subj,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "        \"n_samples\": len(g),\n",
    "    })\n",
    "\n",
    "df_subject_metrics = pd.DataFrame(rows_subj)\n",
    "\n",
    "subj_metrics_file = os.path.join(\n",
    "    results_dir,\n",
    "    f\"loso_subject_metrics_perm_RF_{window_size}win.csv\",\n",
    ")\n",
    "df_subject_metrics.to_csv(subj_metrics_file, index=False)\n",
    "\n",
    "print(f\"\\nSaved subject-level RF perm metrics to {subj_metrics_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ==========================\n",
    "# Config & data loading\n",
    "# ==========================\n",
    "data_root = \"/gpfs1/pi/anon/mindless_reading/data\"\n",
    "sfreq = 256\n",
    "window_seconds = 2\n",
    "window_size = int(sfreq * window_seconds)\n",
    "\n",
    "df = pd.read_csv(\n",
    "    os.path.join(data_root, f\"all_subjects_{window_size}windowed_features.csv\")\n",
    ")\n",
    "df = df.dropna(axis=0, how=\"any\").reset_index(drop=True)\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "pca_variance = 0.95  # keep 95% variance\n",
    "\n",
    "# ==========================\n",
    "# Features & groups\n",
    "# ==========================\n",
    "feature_cols = [c for c in df.columns if c not in [\"label\", \"subject_id\"]]\n",
    "X = df[feature_cols].values\n",
    "y = df[\"label\"].values\n",
    "groups = df[\"subject_id\"].values  # for LOSO\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Random Forest Model\n",
    "# ==========================\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# LOSO CV + logging (EEG + Eye only)\n",
    "# ==========================\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "all_preds = []\n",
    "\n",
    "feature_set = \"EEG + Eye\"\n",
    "X_data = X  # combined features\n",
    "\n",
    "print(f\"\\n========== Feature set: {feature_set} ==========\")\n",
    "\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "prec_list = []\n",
    "auc_list = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_data, y, groups=groups):\n",
    "    subj_test = np.unique(groups[test_idx])[0]\n",
    "\n",
    "    X_train, X_test = X_data[train_idx], X_data[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"pca\", PCA(n_components=pca_variance)),\n",
    "        (\"clf\", rf_clf),\n",
    "    ])\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    # Probabilities → for AUC\n",
    "    try:\n",
    "        y_scores = pipe.predict_proba(X_test)[:, 1]\n",
    "    except Exception:\n",
    "        y_scores = None\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "    auc = (\n",
    "        roc_auc_score(y_test, y_scores)\n",
    "        if (y_scores is not None and len(np.unique(y_test)) == 2)\n",
    "        else np.nan\n",
    "    )\n",
    "\n",
    "    acc_list.append(acc)\n",
    "    f1_list.append(f1)\n",
    "    prec_list.append(prec)\n",
    "    auc_list.append(auc)\n",
    "\n",
    "    print(\n",
    "        f\"  Subject {subj_test}: \"\n",
    "        f\"acc={acc:.3f}, prec={prec:.3f}, f1={f1:.3f}, \"\n",
    "        f\"auc={auc if not np.isnan(auc) else float('nan'):.3f}\"\n",
    "    )\n",
    "\n",
    "    # store sample-level predictions\n",
    "    for i, idx in enumerate(test_idx):\n",
    "        all_preds.append({\n",
    "            \"feature_set\": feature_set,\n",
    "            \"model\": \"random_forest\",\n",
    "            \"test_subject\": subj_test,\n",
    "            \"sample_idx\": int(idx),\n",
    "            \"y_true\": int(y_test[i]),\n",
    "            \"y_pred\": int(y_pred[i]),\n",
    "            \"y_score\": float(y_scores[i]) if y_scores is not None else np.nan,\n",
    "        })\n",
    "\n",
    "print(\n",
    "    f\"Mean over subjects — acc={np.mean(acc_list):.3f}, \"\n",
    "    f\"prec={np.mean(prec_list):.3f}, \"\n",
    "    f\"f1={np.mean(f1_list):.3f}, \"\n",
    "    f\"auc={np.nanmean(auc_list):.3f}\"\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# Save predictions\n",
    "# ==========================\n",
    "df_preds = pd.DataFrame(all_preds)\n",
    "\n",
    "results_dir = os.path.join(data_root, \"ml_results\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "pred_file = os.path.join(\n",
    "    results_dir,\n",
    "    f\"loso_predictions_RF_{window_size}win.csv\",\n",
    ")\n",
    "df_preds.to_csv(pred_file, index=False)\n",
    "print(f\"\\nSaved RF predictions to {pred_file}\")\n",
    "\n",
    "# ==========================\n",
    "# Overall metrics (EEG + Eye only)\n",
    "# ==========================\n",
    "rows = []\n",
    "g = df_preds  # only one feature_set\n",
    "\n",
    "y_true = g[\"y_true\"].values\n",
    "y_pred = g[\"y_pred\"].values\n",
    "y_score = g[\"y_score\"].values\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "auc = (\n",
    "    roc_auc_score(y_true, y_score)\n",
    "    if (np.unique(y_true).size == 2 and not np.all(np.isnan(y_score)))\n",
    "    else np.nan\n",
    ")\n",
    "\n",
    "rows.append({\n",
    "    \"feature_set\": feature_set,\n",
    "    \"model\": \"random_forest\",\n",
    "    \"accuracy\": acc,\n",
    "    \"precision\": prec,\n",
    "    \"f1\": f1,\n",
    "    \"auc\": auc,\n",
    "    \"n_samples\": len(g),\n",
    "})\n",
    "\n",
    "df_metrics = pd.DataFrame(rows)\n",
    "\n",
    "metrics_file = os.path.join(\n",
    "    results_dir,\n",
    "    f\"loso_metrics_RF_{window_size}win.csv\",\n",
    ")\n",
    "df_metrics.to_csv(metrics_file, index=False)\n",
    "\n",
    "print(f\"\\nSaved RF metrics to {metrics_file}\")\n",
    "print(\"\\n=== Summary (EEG + Eye — RF Only) ===\")\n",
    "print(df_metrics)\n",
    "\n",
    "# ==========================\n",
    "# Subject-level metrics\n",
    "# ==========================\n",
    "rows_subj = []\n",
    "for subj, g_subj in df_preds.groupby(\"test_subject\"):\n",
    "    y_true = g_subj[\"y_true\"].values\n",
    "    y_pred = g_subj[\"y_pred\"].values\n",
    "    y_score = g_subj[\"y_score\"].values\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    auc = (\n",
    "        roc_auc_score(y_true, y_score)\n",
    "        if (np.unique(y_true).size == 2 and not np.all(np.isnan(y_score)))\n",
    "        else np.nan\n",
    "    )\n",
    "\n",
    "    rows_subj.append({\n",
    "        \"feature_set\": feature_set,\n",
    "        \"model\": \"random_forest\",\n",
    "        \"subject_id\": subj,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "        \"n_samples\": len(g_subj),\n",
    "    })\n",
    "\n",
    "df_subject_metrics = pd.DataFrame(rows_subj)\n",
    "\n",
    "subj_metrics_file = os.path.join(\n",
    "    results_dir,\n",
    "    f\"loso_subject_metrics_RF_{window_size}win.csv\",\n",
    ")\n",
    "df_subject_metrics.to_csv(subj_metrics_file, index=False)\n",
    "\n",
    "print(f\"\\nSaved subject-level RF metrics to {subj_metrics_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot subject performance (LOSO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"When grouping with a length-1 list-like\",\n",
    "    category=FutureWarning\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Config\n",
    "# ---------------------------\n",
    "file = \"/gpfs1/pi/anon/mindless_reading/data/ml_results/loso_subject_metrics_512win.csv\"\n",
    "output_pdf = \"loso_subject_metric_histograms_512win.pdf\"\n",
    "\n",
    "# ---------------------------\n",
    "# Load data\n",
    "# ---------------------------\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "metrics = [\"accuracy\", \"precision\", \"f1\", \"auc\"]\n",
    "metric_pretty = {\n",
    "    \"accuracy\": \"Accuracy\",\n",
    "    \"precision\": \"Precision\",\n",
    "    \"f1\": \"F1-score\",\n",
    "    \"auc\": \"ROC-AUC\",\n",
    "}\n",
    "\n",
    "feature_order = [\"EEG + Eye\", \"EEG\", \"Eye\"]\n",
    "feature_colors = {\n",
    "    \"EEG + Eye\": \"tab:blue\",\n",
    "    \"EEG\": \"tab:orange\",\n",
    "    \"Eye\": \"tab:green\",\n",
    "}\n",
    "\n",
    "models = df[\"model\"].unique()\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# ---------------------------\n",
    "# Create PDF & write pages\n",
    "# ---------------------------\n",
    "with PdfPages(output_pdf) as pdf:\n",
    "\n",
    "    for model_name in models:\n",
    "        df_model = df[df[\"model\"] == model_name].copy()\n",
    "\n",
    "        # drop rows with all metrics NaN (just in case)\n",
    "        if df_model[metrics].isna().all(axis=1).all():\n",
    "            print(f\"Skipping model {model_name}: no valid metrics.\")\n",
    "            continue\n",
    "\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(20, 4), sharey=False)\n",
    "        fig.suptitle(f\"{model_name} — subject-level metrics\", fontsize=14)\n",
    "\n",
    "        for ax, metric in zip(axes, metrics):\n",
    "            df_m = df_model[~df_model[metric].isna()].copy()\n",
    "            if df_m.empty:\n",
    "                ax.set_visible(False)\n",
    "                continue\n",
    "\n",
    "            # seaborn histogram with hue = feature_set\n",
    "            sns.histplot(\n",
    "                data=df_m[df_m[\"feature_set\"].isin(feature_order)],\n",
    "                x=metric,\n",
    "                hue=\"feature_set\",\n",
    "                hue_order=feature_order,\n",
    "                palette=feature_colors,\n",
    "                bins=10,\n",
    "                stat=\"count\",\n",
    "                common_norm=False,\n",
    "                alpha=0.4,\n",
    "                edgecolor=\"black\",\n",
    "                ax=ax,\n",
    "                element=\"poly\",\n",
    "            )\n",
    "\n",
    "            # Add mean lines per feature_set\n",
    "            for feat in feature_order:\n",
    "                vals = df_m.loc[df_m[\"feature_set\"] == feat, metric].values\n",
    "                if len(vals) == 0:\n",
    "                    continue\n",
    "                mean_val = np.mean(vals)\n",
    "                ax.axvline(\n",
    "                    mean_val,\n",
    "                    linestyle=\"--\",\n",
    "                    linewidth=2,\n",
    "                    color=feature_colors.get(feat, \"black\"),\n",
    "                    label=f\"{feat} mean={mean_val:.3f}\",\n",
    "                )\n",
    "\n",
    "            ax.set_title(metric_pretty.get(metric, metric))\n",
    "            ax.set_xlabel(metric_pretty.get(metric, metric))\n",
    "            ax.set_ylabel(\"Number of subjects\")\n",
    "\n",
    "            # Avoid duplicate legend entries from histplot + mean lines\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "            # Deduplicate while preserving order\n",
    "            seen = set()\n",
    "            uniq_handles = []\n",
    "            uniq_labels = []\n",
    "            for h, l in zip(handles, labels):\n",
    "                if l not in seen:\n",
    "                    seen.add(l)\n",
    "                    uniq_handles.append(h)\n",
    "                    uniq_labels.append(l)\n",
    "            ax.legend(uniq_handles, uniq_labels, fontsize=8)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "print(f\"\\nSaved all plots to:\\n  {output_pdf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ==========================\n",
    "# Config & data loading\n",
    "# ==========================\n",
    "data_root = \"/gpfs1/pi/anon/mindless_reading/data\"\n",
    "sfreq = 256\n",
    "window_seconds = 2\n",
    "window_size = int(sfreq * window_seconds)\n",
    "\n",
    "df = pd.read_csv(\n",
    "    os.path.join(data_root, f\"all_subjects_{window_size}windowed_features.csv\")\n",
    ")\n",
    "df = df.dropna(axis=0, how=\"any\").reset_index(drop=True)\n",
    "pca_variance = 0.95  # keep 95% variance\n",
    "\n",
    "# ==========================\n",
    "# Features & groups\n",
    "# ==========================\n",
    "feature_cols = [c for c in df.columns if c not in [\"label\", \"subject_id\"]]\n",
    "X = df[feature_cols].values\n",
    "y = df[\"label\"].values\n",
    "groups = df[\"subject_id\"].values  # for LOSO\n",
    "\n",
    "# Standardize + PCA (fit on all data for visualization)\n",
    "scaler = StandardScaler()\n",
    "Xz = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=pca_variance, random_state=42)\n",
    "Z = pca.fit_transform(Xz)   # PC scores: shape (n_samples, n_components)\n",
    "\n",
    "labels = np.asarray(y)\n",
    "classes = np.unique(labels)\n",
    "\n",
    "pc_x = 32  # PC33 (0-based index)\n",
    "pc_y = 11  # PC12\n",
    "\n",
    "labels = np.asarray(y)\n",
    "classes = np.unique(labels)\n",
    "\n",
    "# robust zoom limits (ignore extreme outliers)\n",
    "xlo, xhi = np.percentile(Z[:, pc_x], [1, 99])\n",
    "ylo, yhi = np.percentile(Z[:, pc_y], [1, 99])\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "for c in classes:\n",
    "    idx = labels == c\n",
    "    plt.scatter(\n",
    "        Z[idx, pc_x],\n",
    "        Z[idx, pc_y],\n",
    "        s=10,\n",
    "        alpha=0.35,\n",
    "        label=f\"class {c} (n={idx.sum()})\"\n",
    "    )\n",
    "\n",
    "plt.xlim(xlo, xhi)\n",
    "plt.ylim(ylo, yhi)\n",
    "\n",
    "plt.xlabel(f\"PC{pc_x+1} ({pca.explained_variance_ratio_[pc_x]*100:.2f}% var)\")\n",
    "plt.ylabel(f\"PC{pc_y+1} ({pca.explained_variance_ratio_[pc_y]*100:.2f}% var)\")\n",
    "plt.title(f\"PCA: PC{pc_x+1} vs PC{pc_y+1}\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "pc_x = 0  \n",
    "pc_y = 1  \n",
    "\n",
    "labels = np.asarray(y)\n",
    "classes = np.unique(labels)\n",
    "\n",
    "# robust zoom limits (ignore extreme outliers)\n",
    "xlo, xhi = np.percentile(Z[:, pc_x], [1, 99])\n",
    "ylo, yhi = np.percentile(Z[:, pc_y], [1, 99])\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "for c in classes:\n",
    "    idx = labels == c\n",
    "    plt.scatter(\n",
    "        Z[idx, pc_x],\n",
    "        Z[idx, pc_y],\n",
    "        s=10,\n",
    "        alpha=0.35,\n",
    "        label=f\"class {c} (n={idx.sum()})\"\n",
    "    )\n",
    "\n",
    "plt.xlim(xlo, xhi)\n",
    "plt.ylim(ylo, yhi)\n",
    "\n",
    "plt.xlabel(f\"PC{pc_x+1} ({pca.explained_variance_ratio_[pc_x]*100:.2f}% var)\")\n",
    "plt.ylabel(f\"PC{pc_y+1} ({pca.explained_variance_ratio_[pc_y]*100:.2f}% var)\")\n",
    "plt.title(f\"PCA: PC{pc_x+1} vs PC{pc_y+1}\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot EEG features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "# ==========================\n",
    "# Config & data loading\n",
    "# ==========================\n",
    "data_root = \"/gpfs1/pi/anon/mindless_reading/data\"\n",
    "sfreq = 256\n",
    "window_seconds = 2\n",
    "window_size = int(sfreq * window_seconds)\n",
    "\n",
    "df = pd.read_csv(os.path.join(data_root, f\"all_subjects_{window_size}windowed_features.csv\"))\n",
    "df = df.dropna(axis=0, how=\"any\").reset_index(drop=True)\n",
    "\n",
    "n_ch = 64\n",
    "n_band = 8\n",
    "n_eeg = n_ch * n_band\n",
    "\n",
    "# EEG only (assumes first 512 columns are EEG chan_band features)\n",
    "eeg_cols = df.columns[:n_eeg]\n",
    "X = df.loc[:, eeg_cols].to_numpy(dtype=float)\n",
    "y = df[\"label\"].to_numpy()\n",
    "\n",
    "# ==========================\n",
    "# Parse channel & band names\n",
    "# ==========================\n",
    "ch_names, band_names = [], []\n",
    "for c in eeg_cols:\n",
    "    ch, band = c.split(\"_\", 1)\n",
    "    if ch not in ch_names:\n",
    "        ch_names.append(ch)\n",
    "    if band not in band_names:\n",
    "        band_names.append(band)\n",
    "\n",
    "assert len(ch_names) == n_ch, f\"Expected {n_ch} channels, got {len(ch_names)}\"\n",
    "assert len(band_names) == n_band, f\"Expected {n_band} bands, got {len(band_names)}\"\n",
    "\n",
    "# (Optional) enforce canonical band order if you want:\n",
    "canonical_band_order = [\"theta1\",\"theta2\",\"alpha1\",\"alpha2\",\"beta1\",\"beta2\",\"gamma1\",\"gamma2\"]\n",
    "if set(canonical_band_order) == set(band_names):\n",
    "    band_names = canonical_band_order  # reorder labels ONLY (see note below)\n",
    "\n",
    "# ==========================\n",
    "# Compute class difference: (class1 - class0)\n",
    "# ==========================\n",
    "X3 = X.reshape(-1, n_ch, n_band)  # (n_samples, 64, 8)\n",
    "\n",
    "X0 = X3[y == 0]\n",
    "X1 = X3[y == 1]\n",
    "\n",
    "mean_diff = X1.mean(axis=0) - X0.mean(axis=0)  # (64, 8)\n",
    "\n",
    "# ==========================\n",
    "# Topomaps per band\n",
    "# ==========================\n",
    "rename_map = {\"Afz\": \"AFz\"}\n",
    "ch_names_fixed = [rename_map.get(ch, ch) for ch in ch_names]\n",
    "info = mne.create_info(ch_names=ch_names_fixed, sfreq=sfreq, ch_types=\"eeg\")\n",
    "montage = mne.channels.make_standard_montage(\"biosemi64\")\n",
    "info.set_montage(montage)\n",
    "\n",
    "# robust symmetric color limits\n",
    "# vmax = np.percentile(np.abs(mean_diff), 95)\n",
    "# vlim = (-vmax, vmax)\n",
    "\n",
    "# def add_colorbar(fig, vmin, vmax, label):\n",
    "#     sm = ScalarMappable(norm=Normalize(vmin=vmin, vmax=vmax), cmap=\"RdBu_r\")\n",
    "#     sm.set_array([])\n",
    "#     cbar = fig.colorbar(\n",
    "#         sm, ax=fig.axes, orientation=\"vertical\",\n",
    "#         fraction=0.02, pad=0.02\n",
    "#     )\n",
    "#     cbar.set_label(label)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "for b, band in enumerate(band_names):\n",
    "    ax = axes.flat[b]\n",
    "    im, _ = mne.viz.plot_topomap(\n",
    "        X0.mean(axis=0)[:, b],\n",
    "        info,\n",
    "        axes=axes.flat[b],\n",
    "        show=False,\n",
    "        cmap=\"RdBu_r\",\n",
    "        contours=0\n",
    "    )\n",
    "    axes.flat[b].set_title(band)\n",
    "    fig.colorbar(im, ax=axes.flat[b], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.suptitle(\"Class 0: Normal Reading\", y=1.02)\n",
    "# add_colorbar(fig, vmin, vmax, \"EEG band power\")\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "for b, band in enumerate(band_names):\n",
    "    ax = axes.flat[b]\n",
    "    im, _ = mne.viz.plot_topomap(\n",
    "        X1.mean(axis=0)[:, b],\n",
    "        info,\n",
    "        axes=axes.flat[b],\n",
    "        show=False,\n",
    "        cmap=\"RdBu_r\",\n",
    "        contours=0\n",
    "    )\n",
    "    axes.flat[b].set_title(band)\n",
    "    fig.colorbar(im, ax=axes.flat[b], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.suptitle(\"Class 1: Mind-Wandering\", y=1.02)\n",
    "# add_colorbar(fig, vmin, vmax, \"EEG band power\")\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "for b, band in enumerate(band_names):\n",
    "    ax = axes.flat[b]\n",
    "    im, _ = mne.viz.plot_topomap(\n",
    "        mean_diff[:, b],\n",
    "        info,\n",
    "        axes=axes.flat[b],\n",
    "        show=False,\n",
    "        cmap=\"RdBu_r\",\n",
    "        contours=0\n",
    "    )\n",
    "    axes.flat[b].set_title(band)\n",
    "    fig.colorbar(im, ax=axes.flat[b], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.suptitle(\"Topographic difference (Class 1 − Class 0)\", y=1.02)\n",
    "# add_colorbar(fig, vmin, vmax, \"Δ EEG band power\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner EEG features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ==========================\n",
    "# Config & data loading\n",
    "# ==========================\n",
    "data_root = \"/gpfs1/pi/anon/mindless_reading/data\"\n",
    "sfreq = 256\n",
    "window_seconds = 2\n",
    "window_size = int(sfreq * window_seconds)\n",
    "\n",
    "df = pd.read_csv(\n",
    "    os.path.join(data_root, f\"all_subjects_{window_size}windowed_features.csv\")\n",
    ")\n",
    "df = df.dropna(axis=0, how=\"any\").reset_index(drop=True)\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "pca_variance = 0.95  # keep 95% variance\n",
    "\n",
    "# drop outer EEG channels\n",
    "outer_channels_biosemi64 = [\n",
    "    # frontal\n",
    "    \"Fp1\", \"Fp2\", \"AF7\", \"AF8\", \"AF3\", \"AF4\", \"Afz\",\n",
    "    # left temporal\n",
    "    \"FT7\", \"F7\", \"T7\", \"TP7\",\n",
    "    # right temporal\n",
    "    \"FT8\", \"F8\", \"T8\", \"TP8\",\n",
    "    # posterior\n",
    "    \"PO7\", \"PO8\", \"O1\", \"Oz\", \"O2\",\n",
    "    # inferior parietal\n",
    "    \"P7\", \"P8\",\n",
    "]\n",
    "# build tuple of channel prefixes like \"Fp1_\", \"AF7_\", etc.\n",
    "outer_prefixes = tuple(f\"{ch}_\" for ch in outer_channels_biosemi64)\n",
    "# columns to drop\n",
    "cols_to_drop = [c for c in df.columns if c.startswith(outer_prefixes)]\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "# ==========================\n",
    "# Features & groups\n",
    "# ==========================\n",
    "feature_cols = [c for c in df.columns if c not in [\"label\", \"subject_id\"]]\n",
    "X = df[feature_cols].values\n",
    "y = df[\"label\"].values\n",
    "groups = df[\"subject_id\"].values  # for LOSO\n",
    "\n",
    "# Separate EEG and eye-tracking features\n",
    "# fix_count is the first eye-feature column\n",
    "idx_fix = np.where(df.columns.str.contains(\"fix_count\"))[0][0]\n",
    "\n",
    "X_eeg = X[:, :idx_fix]   # EEG-only features\n",
    "X_eye = X[:, idx_fix:]   # Eye-only features\n",
    "\n",
    "# ==========================\n",
    "# Random Forest Model\n",
    "# ==========================\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# LOSO CV + logging (EEG + Eye only)\n",
    "# ==========================\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "for feature_set, X_data in zip(\n",
    "    [\"EEG + Eye\", \"EEG\", \"Eye\"],\n",
    "    [X, X_eeg, X_eye],\n",
    "):\n",
    "    print(f\"\\n========== Feature set: {feature_set} ==========\")\n",
    "    acc_list = []\n",
    "    f1_list = []\n",
    "    prec_list = []\n",
    "    auc_list = []\n",
    "    for train_idx, test_idx in logo.split(X_data, y, groups=groups):\n",
    "        subj_test = np.unique(groups[test_idx])[0]\n",
    "\n",
    "        X_train, X_test = X_data[train_idx], X_data[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"pca\", PCA(n_components=pca_variance)),\n",
    "            (\"clf\", rf_clf),\n",
    "        ])\n",
    "\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_pred = pipe.predict(X_test)\n",
    "\n",
    "        # Probabilities → for AUC\n",
    "        try:\n",
    "            y_scores = pipe.predict_proba(X_test)[:, 1]\n",
    "        except Exception:\n",
    "            y_scores = None\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "        auc = (\n",
    "            roc_auc_score(y_test, y_scores)\n",
    "            if (y_scores is not None and len(np.unique(y_test)) == 2)\n",
    "            else np.nan\n",
    "        )\n",
    "\n",
    "        acc_list.append(acc)\n",
    "        f1_list.append(f1)\n",
    "        prec_list.append(prec)\n",
    "        auc_list.append(auc)\n",
    "\n",
    "        print(\n",
    "            f\"  Subject {subj_test}: \"\n",
    "            f\"acc={acc:.3f}, prec={prec:.3f}, f1={f1:.3f}, \"\n",
    "            f\"auc={auc if not np.isnan(auc) else float('nan'):.3f}\"\n",
    "        )\n",
    "\n",
    "        # # store sample-level predictions\n",
    "        # for i, idx in enumerate(test_idx):\n",
    "        #     all_preds.append({\n",
    "        #         \"feature_set\": feature_set,\n",
    "        #         \"model\": \"random_forest\",\n",
    "        #         \"test_subject\": subj_test,\n",
    "        #         \"sample_idx\": int(idx),\n",
    "        #         \"y_true\": int(y_test[i]),\n",
    "        #         \"y_pred\": int(y_pred[i]),\n",
    "        #         \"y_score\": float(y_scores[i]) if y_scores is not None else np.nan,\n",
    "        #     })\n",
    "\n",
    "    print(\n",
    "        f\"Mean over subjects — acc={np.mean(acc_list):.3f}, \"\n",
    "        f\"prec={np.mean(prec_list):.3f}, \"\n",
    "        f\"f1={np.mean(f1_list):.3f}, \"\n",
    "        f\"auc={np.nanmean(auc_list):.3f}\"\n",
    "    )\n",
    "\n",
    "# # ==========================\n",
    "# # Save predictions\n",
    "# # ==========================\n",
    "# df_preds = pd.DataFrame(all_preds)\n",
    "\n",
    "# results_dir = os.path.join(data_root, \"ml_results\")\n",
    "# os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# pred_file = os.path.join(\n",
    "#     results_dir,\n",
    "#     f\"loso_predictions_RF_{window_size}win.csv\",\n",
    "# )\n",
    "# df_preds.to_csv(pred_file, index=False)\n",
    "# print(f\"\\nSaved RF predictions to {pred_file}\")\n",
    "\n",
    "# # ==========================\n",
    "# # Overall metrics (EEG + Eye only)\n",
    "# # ==========================\n",
    "# rows = []\n",
    "# g = df_preds  # only one feature_set\n",
    "\n",
    "# y_true = g[\"y_true\"].values\n",
    "# y_pred = g[\"y_pred\"].values\n",
    "# y_score = g[\"y_score\"].values\n",
    "\n",
    "# acc = accuracy_score(y_true, y_pred)\n",
    "# f1 = f1_score(y_true, y_pred)\n",
    "# prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "# auc = (\n",
    "#     roc_auc_score(y_true, y_score)\n",
    "#     if (np.unique(y_true).size == 2 and not np.all(np.isnan(y_score)))\n",
    "#     else np.nan\n",
    "# )\n",
    "\n",
    "# rows.append({\n",
    "#     \"feature_set\": feature_set,\n",
    "#     \"model\": \"random_forest\",\n",
    "#     \"accuracy\": acc,\n",
    "#     \"precision\": prec,\n",
    "#     \"f1\": f1,\n",
    "#     \"auc\": auc,\n",
    "#     \"n_samples\": len(g),\n",
    "# })\n",
    "\n",
    "# df_metrics = pd.DataFrame(rows)\n",
    "\n",
    "# metrics_file = os.path.join(\n",
    "#     results_dir,\n",
    "#     f\"loso_metrics_RF_{window_size}win.csv\",\n",
    "# )\n",
    "# df_metrics.to_csv(metrics_file, index=False)\n",
    "\n",
    "# print(f\"\\nSaved RF metrics to {metrics_file}\")\n",
    "# print(\"\\n=== Summary (EEG + Eye — RF Only) ===\")\n",
    "# print(df_metrics)\n",
    "\n",
    "# # ==========================\n",
    "# # Subject-level metrics\n",
    "# # ==========================\n",
    "# rows_subj = []\n",
    "# for subj, g_subj in df_preds.groupby(\"test_subject\"):\n",
    "#     y_true = g_subj[\"y_true\"].values\n",
    "#     y_pred = g_subj[\"y_pred\"].values\n",
    "#     y_score = g_subj[\"y_score\"].values\n",
    "\n",
    "#     acc = accuracy_score(y_true, y_pred)\n",
    "#     f1 = f1_score(y_true, y_pred)\n",
    "#     prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "#     auc = (\n",
    "#         roc_auc_score(y_true, y_score)\n",
    "#         if (np.unique(y_true).size == 2 and not np.all(np.isnan(y_score)))\n",
    "#         else np.nan\n",
    "#     )\n",
    "\n",
    "#     rows_subj.append({\n",
    "#         \"feature_set\": feature_set,\n",
    "#         \"model\": \"random_forest\",\n",
    "#         \"subject_id\": subj,\n",
    "#         \"accuracy\": acc,\n",
    "#         \"precision\": prec,\n",
    "#         \"f1\": f1,\n",
    "#         \"auc\": auc,\n",
    "#         \"n_samples\": len(g_subj),\n",
    "#     })\n",
    "\n",
    "# df_subject_metrics = pd.DataFrame(rows_subj)\n",
    "\n",
    "# subj_metrics_file = os.path.join(\n",
    "#     results_dir,\n",
    "#     f\"loso_subject_metrics_RF_{window_size}win.csv\",\n",
    "# )\n",
    "# df_subject_metrics.to_csv(subj_metrics_file, index=False)\n",
    "\n",
    "# print(f\"\\nSaved subject-level RF metrics to {subj_metrics_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roamm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
