{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52c849ac",
   "metadata": {},
   "source": [
    "# Create ROAMM\n",
    "\n",
    "This script is used to generate datasets ready for ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2476be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def speed_pupil(pup, time):\n",
    "    \"\"\"\n",
    "    Compute pupil dilation speed as max(|backward diff|, |forward diff|) per sample.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pup : array-like\n",
    "        Pupil size time series\n",
    "    time : array-like\n",
    "        Time vector (same length as pup)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    speed : np.ndarray\n",
    "        Pupil speed per time point (same length as pup)\n",
    "    \"\"\"\n",
    "    pup = np.asarray(pup, dtype=float)\n",
    "    time = np.asarray(time, dtype=float)\n",
    "\n",
    "    # finite difference\n",
    "    cur_dilation_speed = np.diff(pup) / np.diff(time)\n",
    "\n",
    "    # backward and forward versions\n",
    "    backward_pupil = np.concatenate(([np.nan], cur_dilation_speed))\n",
    "    forward_pupil = np.concatenate((cur_dilation_speed, [np.nan]))\n",
    "\n",
    "    # stack and take max absolute value\n",
    "    A = np.abs(np.column_stack([backward_pupil, forward_pupil]))\n",
    "\n",
    "    valid = np.isfinite(A).any(axis=1)\n",
    "    speed = np.full(A.shape[0], np.nan)\n",
    "    speed[valid] = np.nanmax(A[valid], axis=1)\n",
    "\n",
    "    return speed\n",
    "\n",
    "\n",
    "def calc_mad(max_dilation, n=16):\n",
    "    \"\"\"\n",
    "    Median Absolute Deviation (MAD) threshold:\n",
    "    threshold = median(x) + n * median(|x - median(x)|)\n",
    "    \"\"\"\n",
    "    x = np.asarray(max_dilation, dtype=float)\n",
    "    med = np.nanmedian(x)\n",
    "    mad = np.nanmedian(np.abs(x - med))\n",
    "    return med + n * mad            \n",
    "\n",
    "def process_eeg_data(file_path):\n",
    "    \"\"\"\n",
    "    Process EEG data and extract page start times.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the EEG .set file\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (eeg_data, df_data, page_start_times, sampling_freq)\n",
    "    \"\"\"\n",
    "    # Load EEG file\n",
    "    raw = mne.io.read_raw_eeglab(file_path, preload=True)\n",
    "    ch_names = raw.info['ch_names']\n",
    "    \n",
    "    # Get the data as numpy array\n",
    "    data, times = raw.get_data(return_times=True)\n",
    "    \n",
    "    # Convert to dataframes\n",
    "    eeg_data = pd.DataFrame(data.T, columns=ch_names)\n",
    "    df_data = pd.DataFrame({'time': times})\n",
    "    \n",
    "    # Get sampling frequency and events\n",
    "    fs = raw.info['sfreq']\n",
    "    df_data['sfreq'] = fs\n",
    "    \n",
    "    events, event_id = mne.events_from_annotations(raw)\n",
    "    \n",
    "    # Extract page start times (condition 10 events)\n",
    "    page_start_times = []\n",
    "    \n",
    "    if 'condition 10' in event_id:\n",
    "        condition10_code = event_id['condition 10']\n",
    "        condition10_events = events[events[:, 2] == condition10_code]\n",
    "        \n",
    "        if len(condition10_events) != 10:\n",
    "            raise ValueError(f'Expected 10 pages, found {len(condition10_events)}')\n",
    "        \n",
    "        # Convert sample indices to time\n",
    "        for event in condition10_events:\n",
    "            page_sample = event[0]\n",
    "            page_start_times.append(page_sample / fs)\n",
    "    else:\n",
    "        raise ValueError('condition 10 not found in event_id')\n",
    "    \n",
    "    # Clean up large variables to free memory\n",
    "    del raw, data, times, events\n",
    "    \n",
    "    return pd.concat([eeg_data, df_data], axis=1), page_start_times, fs\n",
    "\n",
    "\n",
    "def interpolate_blink(dfSamples, dfBlink, dfSaccade):\n",
    "    \"\"\"\n",
    "    Interpolate left and right pupil sizes over blink periods. Modifies the\n",
    "    dataframe of samples in place to change pupil dilation values to interpolated\n",
    "    values, effectively removing blink artifacts. Saves interpolated data as csv.\n",
    "    \n",
    "    Uses saccades as t1 and t4. Contains adjustments recommended through conversation\n",
    "    with Dr. J. Performs the interpolation over the normalized pupil dilation values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dfSamples : pandas.DataFrame\n",
    "        Sample-level eye data containing timestamp column `tSample` and\n",
    "        columns for each eye named like `LX`, `LY`, `LPupil`, `RX`, `RY`, `RPupil`.\n",
    "    dfBlink : pandas.DataFrame\n",
    "        Blink events with columns `tStart`, `tEnd`, and `eye` (values 'L'/'R').\n",
    "    dfSaccade : pandas.DataFrame\n",
    "        Saccade events with columns `tStart`, `tEnd`, and `eye` used to\n",
    "        identify saccades that overlap or surround blinks.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The input `dfSamples` with pupil and position columns replaced by\n",
    "        interpolated values during blink-related intervals.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Interpolation points are chosen using saccades that overlap the blink\n",
    "      when available; otherwise the nearest surrounding saccades are used.\n",
    "    - Interpolation is performed independently for position (`X`,`Y`) and\n",
    "      pupil size columns for each eye.\n",
    "    \"\"\"\n",
    "    # extracted from reading_analysis.py (author: HS)\n",
    "    # interpolate the pupil size during the blink duration\n",
    "    # http://dx.doi.org/10.6084/m9.figshare.688002\n",
    "    \n",
    "\n",
    "    # get time array from dfSamples\n",
    "    sample_time = dfSamples['tSample'].to_numpy()\n",
    "\n",
    "    # interpolate data for LEFT and RIGHT eye separately\n",
    "    for eye in ['L', 'R']:\n",
    "        # extract blink and saccade information for one eye\n",
    "        dfBlink_ = dfBlink[dfBlink['eye']==eye]\n",
    "        dfSaccade_ = dfSaccade[dfSaccade['eye']==eye]\n",
    "\n",
    "        # truncate blink dataframe using the saccade information\n",
    "        t_start = dfSaccade_['tStart'].min()\n",
    "        t_end = dfSaccade_['tEnd'].max()\n",
    "        mask = (dfBlink_['tStart'] > t_start) & (dfBlink_['tEnd'] < t_end)\n",
    "        dfBlink_ = dfBlink_[mask]\n",
    "\n",
    "        # convert df columns to np.arrays for interpolation\n",
    "        col_names = [f'{eye}X', f'{eye}Y', f'{eye}Pupil']\n",
    "        data_to_interpolate = []\n",
    "        for col_name in col_names:\n",
    "            data_to_interpolate.append(np.array(dfSamples[col_name]))\n",
    "\n",
    "        # iterate throu each row of blink dataframe\n",
    "        for index in np.arange(len(dfBlink_)):\n",
    "            row = dfBlink_.iloc[index]\n",
    "            # get the start and end time\n",
    "            b_start = row['tStart'] \n",
    "            b_end = row['tEnd']\n",
    "            # skip blinks out of range of dfSamples\n",
    "            if (b_start < sample_time[0]) and (b_end > sample_time[-1]):\n",
    "                continue\n",
    "            \n",
    "            # commented out by HS on 12/29/2025\n",
    "            # # set t1 to be the end time of the last saccade before the blink\n",
    "            # #get all saccades before this blink\n",
    "            # previous_sac = dfSaccade_[dfSaccade_[\"tEnd\"] < b_start]\n",
    "            # # get last saccade before this blink\n",
    "            # t1 = previous_sac[\"tEnd\"].max()\n",
    "            # # set t2 to be the start time of the first saccade after the blink\n",
    "            # # get all saccades after this blink\n",
    "            # after_sac = dfSaccade_[dfSaccade_[\"tStart\"] > b_end]\n",
    "            # # get the first saccade after this blink\n",
    "            # t2 = after_sac[\"tStart\"].min()\n",
    "\n",
    "            # 12/29/2025 - added by HS\n",
    "            # set t1 and t2 to be the start and end time of the saccade that surrounds the blink\n",
    "            # this is to avoid the long fixation between saccades that may lead to large interpolation errors\n",
    "\n",
    "            # saccades that overlap the blink\n",
    "            sac = dfSaccade_[\n",
    "                (dfSaccade_[\"tStart\"] < b_start) &\n",
    "                (dfSaccade_[\"tEnd\"] > b_end)\n",
    "            ]\n",
    "\n",
    "            if not sac.empty:\n",
    "                # use overlapping saccade\n",
    "                t1 = sac[\"tStart\"].iloc[-1]\n",
    "                t2 = sac[\"tEnd\"].iloc[-1]\n",
    "\n",
    "            else:\n",
    "                # previous saccade before blink\n",
    "                previous_sac = dfSaccade_[dfSaccade_[\"tEnd\"] < b_start]\n",
    "                if previous_sac.empty:\n",
    "                    t1 = np.nan\n",
    "                    raise ValueError(\"t1 are Na\")\n",
    "                else:\n",
    "                    t1 = previous_sac[\"tEnd\"].max()\n",
    "\n",
    "                # first saccade after blink\n",
    "                after_sac = dfSaccade_[dfSaccade_[\"tStart\"] > b_end]\n",
    "                if after_sac.empty:\n",
    "                    t2 = np.nan\n",
    "                    raise ValueError(\"t2 are Na\")\n",
    "                else:\n",
    "                    t2 = after_sac[\"tStart\"].min()\n",
    "\n",
    "            # check for missing vals in t1 or t2 and use fallback if needed\n",
    "            # if pd.isna(t1) or pd.isna(t2):\n",
    "            #     raise ValueError(\"t1/t2 are Na\")\n",
    "            \n",
    "            # check the timing of saccades are within the time array for samples\n",
    "            if (t1 > sample_time[0]) and (t2 < sample_time[-1]):\n",
    "                # choose data points for interpolation function\n",
    "                x = [t1,t2]\n",
    "                y_ind = []\n",
    "                for t in x:\n",
    "                    y_ind.append(np.where(sample_time==t)[0][0])\n",
    "\n",
    "                # loop thru all columns\n",
    "                for col_name, col_data in zip(col_names, data_to_interpolate):\n",
    "                    # create the 1D function for interpolation\n",
    "                    y = col_data[y_ind]\n",
    "                    interp_f = interp1d(x, y)           \n",
    "                    #spl = CubicSpline(x, y)\n",
    "                    \n",
    "                    # generate mask for blink duration\n",
    "                    mask = (sample_time > t1) & (sample_time < t2)\n",
    "                    time_to_interpolate = sample_time[mask]\n",
    "                    # use spl model to interpolate data during blink duration\n",
    "                    interp_data = interp_f(time_to_interpolate)\n",
    "                    \n",
    "                    # update the dfSamples in place\n",
    "                    dfSamples.loc[mask, col_name] = interp_data\n",
    "\n",
    "    return dfSamples\n",
    "\n",
    "\n",
    "def preprocess_pupil(dfSamples, dfBlink):\n",
    "    \"\"\"\n",
    "    Preprocess pupil data by removing blinks and interpolating across them.\n",
    "\n",
    "    Performs the following steps:\n",
    "    - Marks sample data as NaN during blink periods (with 100 ms padding)\n",
    "    - Removes outliers from pupil size using Median Absolute Deviation (MAD)\n",
    "    - Performs linear interpolation across gaps in the data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dfSamples : pandas.DataFrame\n",
    "        Sample-level eye data containing columns `tSample`, `LX`, `LY`, `LPupil`,\n",
    "        `RX`, `RY`, `RPupil`.\n",
    "    dfBlink : pandas.DataFrame\n",
    "        Blink events with columns `tStart`, `tEnd` (milliseconds), and `eye` ('L'/'R').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The input `dfSamples` with pupil and position data cleaned by removing\n",
    "        blinks and interpolating across them.\n",
    "    \"\"\"\n",
    "    # set data during blink to NaN\n",
    "    padding = 100 # ms padding before and after blink\n",
    "    for eye in ['L', 'R']:\n",
    "        eye_cols = [f\"{eye}X\", f\"{eye}Y\", f\"{eye}Pupil\"]\n",
    "\n",
    "        # blinks for this eye\n",
    "        df_blink_eye = dfBlink[dfBlink['eye'] == eye]\n",
    "        if df_blink_eye.empty:\n",
    "            continue\n",
    "\n",
    "        # mask out blink + padding\n",
    "        for _, row in df_blink_eye.iterrows():\n",
    "            t_start_pad = row[\"tStart\"] - padding\n",
    "            t_end_pad   = row[\"tEnd\"]   + padding\n",
    "\n",
    "            mask = (dfSamples[\"tSample\"] >= t_start_pad) & (dfSamples[\"tSample\"] <= t_end_pad)\n",
    "            dfSamples.loc[mask, eye_cols] = np.nan\n",
    "        \n",
    "        # linear interpolation for each channel of this eye\n",
    "        for col in eye_cols:\n",
    "            if dfSamples[col].notna().sum() < 2:\n",
    "                continue\n",
    "            # remove outliers for pupil size before interpolation using MAD\n",
    "            if col == f\"{eye}Pupil\":\n",
    "                pupil_speed = speed_pupil(dfSamples[col], dfSamples[\"tSample\"])\n",
    "                speed_threshold = calc_mad(pupil_speed, n=16)\n",
    "                outlier_mask = pupil_speed > speed_threshold\n",
    "                dfSamples.loc[outlier_mask, col] = np.nan\n",
    "\n",
    "            dfSamples[col] = dfSamples[col].interpolate(\n",
    "                method='linear',\n",
    "                limit_area='inside'  # only fill internal gaps\n",
    "            )\n",
    "\n",
    "    return dfSamples\n",
    "\n",
    "\n",
    "def convert_eyelink_to_image_pixel(x_eyelink, y_eyelink):\n",
    "        '''\n",
    "        Convert eyeylink coords to pixel for image (1900 x 1442 pixels) \n",
    "        displayed at pos (0, 0) w/ size (1.3, 0.99) in PsychoPy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_eyelink : float\n",
    "            DESCRIPTION. eyelink coord unit\n",
    "        y_eyelink : float\n",
    "            DESCRIPTION. eyelink coord unit\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_pixel : float\n",
    "            DESCRIPTION. Image pixel unit\n",
    "        y_pixel : float \n",
    "            DESCRIPTION. Image pixel unit \n",
    "\n",
    "        '''\n",
    "        x_pixel = (x_eyelink-258) * 1900 / (1080*1.3)\n",
    "        y_pixel = (y_eyelink-5.4) * 1442 / (1080*0.99)\n",
    "        return x_pixel, y_pixel\n",
    "\n",
    "\n",
    "def find_match(dfWords, coord_info, dist_max=1000):\n",
    "    '''\n",
    "    Match the clicks to words. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dfWords : DataFrame\n",
    "        DESCRIPTION. The dataframe for a single page. It should at least contain\n",
    "        columns 'center_x', 'center_y', 'width', and 'height', which are \n",
    "        coordinate information of words in pixel unit\n",
    "    click_pos : Tuple\n",
    "        DESCRIPTION. The coordinate info (x, y) for a click in pixel unit\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matched_index: int\n",
    "        DESCRIPTION. The index value of matched word for input dataframe. \n",
    "\n",
    "    '''\n",
    "    # compute the start and end positions of words\n",
    "    words_x_start = dfWords['center_x'] - dfWords['width']/2\n",
    "    words_x_end = dfWords['center_x'] + dfWords['width']/2\n",
    "    words_y_start = dfWords['center_y'] - dfWords['height']/2\n",
    "    words_y_end = dfWords['center_y'] + dfWords['height']/2\n",
    "    \n",
    "    # get x and y for the click\n",
    "    pos_x, pos_y = coord_info\n",
    "    \n",
    "    # compute the distance between click and word boundry box\n",
    "    dist_x_left = (words_x_start - pos_x)\n",
    "    dist_x_right = (pos_x - words_x_end)\n",
    "    dist_y_top = (words_y_start - pos_y)\n",
    "    dist_y_bottom = (pos_y - words_y_end)\n",
    "    \n",
    "    # find the maximum distance from click to the word for x and y\n",
    "    max_x = np.max(np.vstack((dist_x_left, dist_x_right, np.zeros(len(dist_x_left)))), axis=0)\n",
    "    max_y = np.max(np.vstack((dist_y_top, dist_y_bottom, np.zeros(len(dist_y_top)))), axis=0)\n",
    "    \n",
    "    # calculate the distance using x and y\n",
    "    dist = np.sqrt(np.square(max_x) + np.square(max_y))\n",
    "    \n",
    "    # check if the minimum dist exceeds threshold values\n",
    "    if np.min(dist) < dist_max:\n",
    "        matched_index = np.argmin(dist)\n",
    "    else:\n",
    "        matched_index = -1\n",
    "    \n",
    "    # return the index that has the shortest distance\n",
    "    return matched_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83631e4",
   "metadata": {},
   "source": [
    "Get all subject folders from the root directory on VACC (used for debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34be240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "root_dir = r\"/gpfs1/pi/djangraw/mindless_reading/data/\"\n",
    "# Regular expression to match subject folder names (s followed by exactly 5 digits)\n",
    "subject_pattern = re.compile(r\"^s\\d{5}$\")\n",
    "\n",
    "# Get the list of subject folders that match the pattern\n",
    "subjects = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d)) and subject_pattern.match(d)]\n",
    "\n",
    "# define res path (relative)\n",
    "res_path = r\"res/\"\n",
    "\n",
    "for sub_id in sorted(subjects):\n",
    "    # if int(sub_id[1:]) < 10115:  # debug purpose\n",
    "    #     continue\n",
    "    print('Start processing subject: ', sub_id)\n",
    "    subject_path = os.path.join(root_dir, sub_id)\n",
    "    # define path to save\n",
    "    path_to_save = os.path.join(subject_path, 'ml_data')\n",
    "    os.makedirs(path_to_save, exist_ok=True)\n",
    "\n",
    "    # load subject page info dataframe\n",
    "    file_path = os.path.join(subject_path, f'{sub_id}_R_features_default_sr.csv')\n",
    "    df_page_info = pd.read_csv(file_path)\n",
    "\n",
    "    # load raw eye datasets for each run\n",
    "    eye_folder = os.path.join(subject_path, 'eye')\n",
    "    folders = [f for f in os.listdir(eye_folder) if os.path.isdir(os.path.join(eye_folder, f))]\n",
    "    r_folders = [f for f in folders if re.search(r'_r[1-5]_', f)]\n",
    "    r_folders_sorted = sorted(r_folders, key=lambda x: int(re.search(r'_r(\\d+)_', x).group(1)))\n",
    "\n",
    "    for folder in r_folders_sorted:\n",
    "        folder_path = os.path.join(eye_folder, folder)\n",
    "        # get run number\n",
    "        run_num = int(re.search(r'_r(\\d+)_', folder).group(1))\n",
    "        print('Run number: ', run_num)\n",
    "\n",
    "        # load EEG file for the current run\n",
    "        file_path = os.path.join(subject_path, 'eeg', 'ICAPruned', f'MR_{sub_id}_r{run_num}_ICAPruned.set')\n",
    "        df_data, page_start_eeg, fs = process_eeg_data(file_path)\n",
    "\n",
    "        # get the current run info\n",
    "        df_page_run = df_page_info[df_page_info['run']==run_num].copy()\n",
    "        eyelink_start_time = df_page_run['task_start'].iloc[0]\n",
    "        # page start time in eyelink\n",
    "        page_start_eye = df_page_run['page_start'] - eyelink_start_time\n",
    "\n",
    "        # make sure time offsets between eeg and eyelink are consistent\n",
    "        time_offsets = np.array(page_start_eeg) - np.array(page_start_eye)\n",
    "        max_offset_allowed = 0.05\n",
    "        if np.max(time_offsets) - np.min(time_offsets) > max_offset_allowed:\n",
    "            raise ValueError('Time offsets between EEG and eyelink not consistent.')\n",
    "        # use the mean to be the time offset (in seconds) for the current run\n",
    "        time_offset = np.mean(time_offsets)\n",
    "\n",
    "        # update eyelink time to match df_data array\n",
    "        df_page_run['page_start_eeg'] = df_page_run['page_start'] - eyelink_start_time + time_offset\n",
    "        df_page_run['page_end_eeg'] = df_page_run['page_end'] - eyelink_start_time + time_offset\n",
    "        df_page_run['mw_onset'] = df_page_run['mw_onset'] - eyelink_start_time + time_offset\n",
    "        df_page_run['mw_offset'] = df_page_run['mw_offset'] - eyelink_start_time + time_offset\n",
    "\n",
    "        # get story name\n",
    "        story_name = df_page_run['reading'].iloc[0]\n",
    "        story_name = story_name.lower().replace(' ', '_')\n",
    "\n",
    "        # load word coordinate file\n",
    "        csv_pattern = os.path.join(res_path, f'{story_name}*.csv')\n",
    "        matching_files = glob.glob(csv_pattern)\n",
    "        df_word = pd.read_csv(matching_files[0])\n",
    "\n",
    "        # load fixation file\n",
    "        csv_pattern = os.path.join(folder_path, f'*Fixation.csv')\n",
    "        matching_files = glob.glob(csv_pattern)\n",
    "        df_fix = pd.read_csv(matching_files[0])\n",
    "\n",
    "        # load blink file\n",
    "        csv_pattern = os.path.join(folder_path, f'*Blink.csv')\n",
    "        matching_files = glob.glob(csv_pattern)\n",
    "        df_blink = pd.read_csv(matching_files[0])\n",
    "\n",
    "        # load saccade file\n",
    "        csv_pattern = os.path.join(folder_path, f'*Saccade.csv')\n",
    "        matching_files = glob.glob(csv_pattern)\n",
    "        df_sacc = pd.read_csv(matching_files[0])\n",
    "\n",
    "        # load raw eye sample file (gaze position and pupil size)\n",
    "        csv_pattern = os.path.join(folder_path, f'*Sample.csv')\n",
    "        matching_files = glob.glob(csv_pattern)\n",
    "        df_sample = pd.read_csv(matching_files[0])\n",
    "\n",
    "        # interpolate eye samples during blinnk\n",
    "        df_interp_sample = df_sample.copy()\n",
    "        # 1/22/26 use new preprocess_pupil function\n",
    "        # interpolate_blink(df_interp_sample, df_blink, df_sacc)\n",
    "        df_interp_sample = preprocess_pupil(df_interp_sample, df_blink)\n",
    "        # copy interpolated data into the samples dataframe\n",
    "        for col in ['LX', 'LY', 'LPupil', 'RX', 'RY', 'RPupil']:\n",
    "            df_sample[f'blink_interp_{col}'] = df_interp_sample[col]\n",
    "        \n",
    "        # loop through each page for matching fixation and word and inserting page-level information\n",
    "        for _, row in df_page_run.iterrows():\n",
    "            # get the current page number\n",
    "            page_num = row['page']\n",
    "            # get the page number start and end time\n",
    "            page_start = row['page_start'] * 1000\n",
    "            page_end = row['page_end'] * 1000\n",
    "\n",
    "            # match fixation to the reading word\n",
    "            # find the fixation and words for the current page\n",
    "            df_word_page = df_word[df_word['page']==page_num]\n",
    "            mask = (df_fix['tStart'] >= page_start) & (df_fix['tEnd'] <= page_end)\n",
    "            df_fix_page = df_fix[mask]\n",
    "            # loop through each fixation and match it to the closed word \n",
    "            for row_index, fix in df_fix_page.iterrows():\n",
    "                # get the fixtaion position x and y\n",
    "                x_eyelink = fix['xAvg']\n",
    "                y_eyelink = fix['yAvg']\n",
    "                # convert position x and y into image pixel unit\n",
    "                fix_x, fix_y = convert_eyelink_to_image_pixel(x_eyelink, y_eyelink)\n",
    "                \n",
    "                # call function find the matched index\n",
    "                matched_index = find_match(df_word_page, (fix_x, fix_y))\n",
    "                \n",
    "                if matched_index >= 0:\n",
    "                    # store matched word info into fixations \n",
    "                    df_fix.at[row_index, 'fixed_word'] = df_word_page['words'].iloc[matched_index]\n",
    "                    df_fix.at[row_index, 'fixed_word_key'] = df_word_page['word_key'].iloc[matched_index]\n",
    "        \n",
    "\n",
    "            # insert page start and end time to df_data\n",
    "            page_mask = (df_data['time'] >= row['page_start_eeg']) & (df_data['time'] <= row['page_end_eeg'])\n",
    "            df_data.loc[page_mask, 'first_pass_reading'] = True\n",
    "            df_data.loc[page_mask, 'page_num'] = page_num\n",
    "            df_data.loc[page_mask, 'page_start'] = row['page_start_eeg']\n",
    "            df_data.loc[page_mask, 'page_end'] = row['page_end_eeg']\n",
    "            df_data.loc[page_mask, 'page_dur'] = row['page_end_eeg'] - row['page_start_eeg']\n",
    "            # insert mw start and end time to df_data\n",
    "            mw_mask = (df_data['time'] >= row['mw_onset']) & (df_data['time'] <= row['mw_offset'])\n",
    "            df_data.loc[mw_mask, 'is_mw'] = True\n",
    "            df_data.loc[mw_mask, 'mw_onset'] = row['mw_onset']\n",
    "            df_data.loc[mw_mask, 'mw_offset'] = row['mw_offset']\n",
    "            df_data.loc[mw_mask, 'mw_dur'] = row['mw_offset'] - row['mw_onset']\n",
    "\n",
    "        print('Aligning eye-tracking data into EEG time...')\n",
    "        # insert current run information\n",
    "        df_data['run_num'] = run_num\n",
    "        df_data['story_name'] = story_name\n",
    "        # sort df_data by time\n",
    "        df_data = df_data.sort_values(\"time\")\n",
    "\n",
    "        # insert fixations, saccades, and blinks to df_data\n",
    "        for df_eye, event_type in zip([df_fix, df_blink, df_sacc], ['fix', 'blink', 'sacc']):\n",
    "            # align fixation time to eeg\n",
    "            # 1. /1000: ms -> s\n",
    "            # 2. - eyelink_start_time: take off the run start time\n",
    "            # 3. + time_offset: add on the time offset between eeg and eye\n",
    "            for col in ['tStart', 'tEnd']:\n",
    "                df_eye[col] = df_eye[col] / 1000 - eyelink_start_time + time_offset\n",
    "                df_eye = df_eye[df_eye['tStart'] >= 0].copy()\n",
    "\n",
    "            # separate left and right eye\n",
    "            for eye in ['L', 'R']:\n",
    "                # Filter by eye and sort\n",
    "                df_eye_filtered = df_eye[df_eye['eye'] == eye].copy().sort_values('tStart')\n",
    "                if len(df_eye_filtered) == 0:\n",
    "                    continue\n",
    "                # Merge on start time\n",
    "                df_merged = pd.merge_asof(\n",
    "                    df_data,\n",
    "                    df_eye_filtered,\n",
    "                    left_on='time',\n",
    "                    right_on='tStart',\n",
    "                    direction='backward',\n",
    "                )\n",
    "                \n",
    "                # Create mask for times within event window (tStart <= time <= tEnd)\n",
    "                mask = (df_merged['tStart'].notna()) & (df_merged['time'] <= df_merged['tEnd'])\n",
    "                # initialize is_{type} column if not exists\n",
    "                if f'is_{event_type}' not in df_data.columns:\n",
    "                    df_data[f'is_{event_type}'] = False\n",
    "                # mark events\n",
    "                df_data.loc[mask, f'is_{event_type}'] = True\n",
    "                # add event columns\n",
    "                for col in df_eye_filtered.columns:\n",
    "                    new_col_name = f'{event_type}_{eye}_{col}'\n",
    "                    df_data.loc[mask, new_col_name] = df_merged.loc[mask, col]\n",
    "\n",
    "        # insert gaze position and pupil size to df_data\n",
    "        df_sample['tSample'] = df_sample['tSample'] / 1000 - eyelink_start_time + time_offset\n",
    "        df_sample = df_sample[df_sample['tSample'] >= 0]\n",
    "        df_sample = df_sample.sort_values('tSample')\n",
    "        # merge on nearest time within tolerance\n",
    "        df_data = pd.merge_asof(\n",
    "            df_data,\n",
    "            df_sample,\n",
    "            left_on='time',\n",
    "            right_on='tSample',\n",
    "            direction='nearest',\n",
    "            tolerance=0.01\n",
    "        )\n",
    "        \n",
    "        # save the dataset for the current run as csv\n",
    "        dataset_name = f'{sub_id}_run{run_num}_ml_data.csv'\n",
    "        df_data.to_csv(os.path.join(path_to_save, dataset_name), index=False)\n",
    "\n",
    "        # save as pickle\n",
    "        dataset_name = f'{sub_id}_run{run_num}_ml_data.pkl'\n",
    "        df_data.to_pickle(os.path.join(path_to_save, dataset_name))\n",
    "\n",
    "        # save dataset during first-pass reading only\n",
    "        dataset_name = f'{sub_id}_run{run_num}_ml_data_firstpass.csv'\n",
    "        df_data_firstpass = df_data[df_data['first_pass_reading']==True].copy()\n",
    "        df_data_firstpass.to_csv(os.path.join(path_to_save, dataset_name), index=False)\n",
    "        \n",
    "        print(f'Subject {sub_id} run {run_num} has done! Data saved to {os.path.join(path_to_save, dataset_name)}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fd4ad4",
   "metadata": {},
   "source": [
    "# Label sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35cd82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "def merge_sentences(sent_texts, merges):\n",
    "    \"\"\"\n",
    "    sent_texts: list[str] original sentences (0-based indexing)\n",
    "    merges: list[list[int]] groups of sentence indices to merge, e.g. [[42,43],[56,57]]\n",
    "\n",
    "    Returns:\n",
    "      sent_texts_corr: list[str]\n",
    "      old2new: dict[int,int] mapping old sentence idx -> new sentence idx\n",
    "    \"\"\"\n",
    "    n = len(sent_texts)\n",
    "\n",
    "    # validate + build groups\n",
    "    merges = [sorted(g) for g in merges]\n",
    "    used = set()\n",
    "    for g in merges:\n",
    "        for i in g:\n",
    "            if i < 0 or i >= n:\n",
    "                raise IndexError(f\"Sentence index {i} out of range (0..{n-1})\")\n",
    "            if i in used:\n",
    "                raise ValueError(f\"Sentence index {i} appears in multiple merge groups\")\n",
    "            used.add(i)\n",
    "\n",
    "    # Build a dict for quick lookup: old idx -> group idx\n",
    "    group_of = {}\n",
    "    for gi, g in enumerate(merges):\n",
    "        for i in g:\n",
    "            group_of[i] = gi\n",
    "\n",
    "    sent_texts_corr = []\n",
    "    old2new = {}\n",
    "\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        if i in group_of:\n",
    "            # merge the whole group starting at its first element\n",
    "            g = merges[group_of[i]]\n",
    "            if i != g[0]:\n",
    "                # we'll skip until we hit the group's first idx\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            merged_text = \" \".join(sent_texts[j] for j in g).strip()\n",
    "            new_idx = len(sent_texts_corr)\n",
    "            sent_texts_corr.append(merged_text)\n",
    "\n",
    "            for j in g:\n",
    "                old2new[j] = new_idx\n",
    "\n",
    "            i = g[-1] + 1\n",
    "        else:\n",
    "            new_idx = len(sent_texts_corr)\n",
    "            sent_texts_corr.append(sent_texts[i])\n",
    "            old2new[i] = new_idx\n",
    "            i += 1\n",
    "\n",
    "    return sent_texts_corr, old2new\n",
    "\n",
    "def row_for_char(c):\n",
    "        \"\"\"\n",
    "        Return df row whose span contains character position c.\n",
    "        If c lands in whitespace (between words), we return the nearest previous row.\n",
    "        \"\"\"\n",
    "        # rightmost span_start <= c\n",
    "        i = np.searchsorted(span_starts, c, side=\"right\") - 1\n",
    "        if i < 0:\n",
    "            return 0\n",
    "        # If inside that span, good\n",
    "        if c < span_ends[i]:\n",
    "            return i\n",
    "        # Otherwise token starts in whitespace; assign to nearest previous row\n",
    "        return i\n",
    "\n",
    "\n",
    "# manual merges by story (0-based indices)\n",
    "MANUAL_MERGES = {\n",
    "    \"history_of_film\": [\n",
    "        [42, 43],\n",
    "        [56, 57],\n",
    "    ],\n",
    "    \"serena_williams\": [\n",
    "        [1, 2],\n",
    "        [3, 4],\n",
    "        [5, 6],\n",
    "        [27, 28],\n",
    "        [49, 50],\n",
    "        [56, 57, 58, 59],\n",
    "        [60, 61],\n",
    "        [62, 63],\n",
    "        [65, 66, 67],\n",
    "        [69, 70],\n",
    "        [75, 76],\n",
    "        [80, 81],\n",
    "        [82, 83],\n",
    "        [86, 87],\n",
    "        [97, 98],\n",
    "        [110, 111],\n",
    "        [113, 114],\n",
    "        [116, 117],\n",
    "    ],\n",
    "}\n",
    "\n",
    "# --------------------\n",
    "# Load spaCy\n",
    "# --------------------\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"tagger\", \"parser\", \"lemmatizer\", \"ner\"])\n",
    "if \"sentencizer\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "root = \"/gpfs1/pi/djangraw/hsun11/roamm_ml/res\"\n",
    "csv_files = sorted(glob(os.path.join(root, \"*.csv\")))\n",
    "\n",
    "out_dir = os.path.join(root, \"with_sentences\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "for filepath in csv_files:\n",
    "    base = os.path.splitext(os.path.basename(filepath))[0].lower()\n",
    "    story_name = base.replace(\"_coordinates\", \"\")\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "    # Original tokens (one per row)\n",
    "    words = df[\"words\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "    # Build joined text AND record char spans for each original word\n",
    "    spans = []  # list of (start_char, end_char) for each df row in the joined string\n",
    "    parts = []\n",
    "    pos = 0\n",
    "    for w in words:\n",
    "        start = pos\n",
    "        parts.append(w)\n",
    "        pos += len(w)\n",
    "        end = pos\n",
    "        spans.append((start, end))\n",
    "        # add the join-space\n",
    "        parts.append(\" \")\n",
    "        pos += 1\n",
    "\n",
    "    text = \"\".join(parts).rstrip()  # remove last space\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # sentence texts (by spaCy segmentation)\n",
    "    sent_texts = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "    if story_name in MANUAL_MERGES:\n",
    "        sent_texts_corr, old2new = merge_sentences(sent_texts, MANUAL_MERGES[story_name])\n",
    "        for g in MANUAL_MERGES[story_name]:\n",
    "            print(\"\\nMERGE:\", g)\n",
    "            for i in g:\n",
    "                print(f\"  [{i}] {sent_texts[i]}\")\n",
    "            merged = \" \".join(sent_texts[i] for i in g)\n",
    "            print(\"  =>\", merged)\n",
    "    else:\n",
    "        sent_texts_corr = sent_texts\n",
    "        old2new = {i: i for i in range(len(sent_texts))}\n",
    "\n",
    "    # --------------------\n",
    "    # Build arrays for fast row lookup\n",
    "    # spans is list[(start,end)] for each df row (word)\n",
    "    # --------------------\n",
    "    span_starts = np.array([s for s, e in spans], dtype=int)\n",
    "    span_ends   = np.array([e for s, e in spans], dtype=int)\n",
    "\n",
    "    # --------------------\n",
    "    # Assign sentence_id to df rows by sentence character spans\n",
    "    # --------------------\n",
    "    sentence_id = np.full(len(df), -1, dtype=int)\n",
    "\n",
    "    for old_sid, sent in enumerate(doc.sents):\n",
    "        # Map old spaCy sentence index -> corrected sentence index (after merges)\n",
    "        new_sid = old2new.get(old_sid, old_sid)\n",
    "\n",
    "        s0 = sent.start_char\n",
    "        s1 = sent.end_char\n",
    "\n",
    "        r0 = row_for_char(s0)\n",
    "        r1 = row_for_char(s1 - 1)  # inclusive end\n",
    "\n",
    "        if r1 < r0:\n",
    "            continue\n",
    "\n",
    "        sentence_id[r0:r1 + 1] = new_sid\n",
    "\n",
    "    # Anything still -1 (rare edge) -> forward fill from previous\n",
    "    # and if still -1 at start -> set to 0\n",
    "    if (sentence_id == -1).any():\n",
    "        for i in range(1, len(sentence_id)):\n",
    "            if sentence_id[i] == -1:\n",
    "                sentence_id[i] = sentence_id[i - 1]\n",
    "        if sentence_id[0] == -1:\n",
    "            sentence_id[0] = 0\n",
    "\n",
    "    df[\"sentence_id\"] = sentence_id\n",
    "    df[\"sentence\"] = df[\"sentence_id\"].map(lambda i: sent_texts_corr[i] if 0 <= i < len(sent_texts_corr) else \"\")\n",
    "    df[\"sentence_id\"] = story_name + \"_\" + df[\"sentence_id\"].astype(str)\n",
    "\n",
    "    df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0536b6db",
   "metadata": {},
   "source": [
    "# Data Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fd7440f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:30<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "root_dir = \"/gpfs1/pi/djangraw/mindless_reading/ROAMM/subject_ml_data/\"\n",
    "subject_pattern = re.compile(r\"^s\\d{5}$\")\n",
    "\n",
    "subjects = sorted(\n",
    "    d for d in os.listdir(root_dir)\n",
    "    if os.path.isdir(os.path.join(root_dir, d)) and subject_pattern.match(d)\n",
    ")\n",
    "\n",
    "rows = []  # per-subject summary rows\n",
    "\n",
    "for sub_id in tqdm.tqdm(subjects):\n",
    "    subject_folder = os.path.join(root_dir, sub_id)\n",
    "    data_files = sorted(glob.glob(os.path.join(subject_folder, \"*.pkl\")))\n",
    "\n",
    "    if len(data_files) == 0:\n",
    "        print(f\"Skipping {sub_id}: no .pkl files found\")\n",
    "        continue\n",
    "\n",
    "    # Counters\n",
    "    total_sample = 0\n",
    "    first_pass_sample = 0\n",
    "    mind_wandering_sample = 0\n",
    "    fixation_sample = 0\n",
    "    saccade_sample = 0\n",
    "    blink_sample = 0\n",
    "\n",
    "    for data_file in data_files:\n",
    "        df_data = pd.read_pickle(data_file)\n",
    "\n",
    "        total_sample += len(df_data)\n",
    "\n",
    "        # robust boolean handling (covers True/False, 0/1, NaN)\n",
    "        def as_bool(s: pd.Series) -> pd.Series:\n",
    "            return pd.to_numeric(s, errors=\"coerce\").fillna(0).astype(int).gt(0)\n",
    "\n",
    "        first_pass_sample += as_bool(df_data[\"first_pass_reading\"]).sum()\n",
    "        mind_wandering_sample += as_bool(df_data[\"is_mw\"]).sum()\n",
    "        fixation_sample += as_bool(df_data[\"is_fix\"]).sum()\n",
    "        saccade_sample += as_bool(df_data[\"is_sacc\"]).sum()\n",
    "        blink_sample += as_bool(df_data[\"is_blink\"]).sum()\n",
    "\n",
    "    rows.append({\n",
    "        \"subject_id\": sub_id,\n",
    "        # \"n_runs\": len(data_files),\n",
    "        \"total_sample\": int(total_sample),\n",
    "        \"first_pass_sample\": int(first_pass_sample),\n",
    "        \"mind_wandering_sample\": int(mind_wandering_sample),\n",
    "        \"fixation_sample\": int(fixation_sample),\n",
    "        \"saccade_sample\": int(saccade_sample),\n",
    "        \"blink_sample\": int(blink_sample),\n",
    "        # \"mw_rate_overall\": float(mind_wandering_sample / total_sample) if total_sample > 0 else np.nan,\n",
    "        # \"mw_rate_first_pass\": float(mind_wandering_sample / first_pass_sample) if first_pass_sample > 0 else np.nan,\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(rows).sort_values(\"subject_id\").reset_index(drop=True)\n",
    "\n",
    "out_csv = os.path.join(root_dir, \"subject_sample_counts_summary.csv\")\n",
    "df_summary.to_csv(out_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33169b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples across all subjects: 46371584\n",
      "Total duration: 50 hours, 18 minutes\n",
      "First-pass reading samples across all subjects: 26691014\n",
      "First-pass reading duration: 28 hours, 57 minutes\n",
      "Mind-wandering samples across all subjects: 2042601\n",
      "Mind-wandering duration: 2 hours, 12 minutes\n",
      "Fixation samples across all subjects: 38316711\n",
      "Fixation duration: 41 hours, 34 minutes\n",
      "Saccade samples across all subjects: 9323085\n",
      "Saccade duration: 10 hours, 6 minutes\n",
      "Blink samples across all subjects: 2287380\n",
      "Blink duration: 2 hours, 28 minutes\n",
      "Average total duration per subject: 1 hours, 8 minutes\n",
      "Average first-pass reading duration per subject: 0 hours, 39 minutes\n",
      "Average mind-wandering duration per subject: 0 hours, 3 minutes\n",
      "Average fixation duration per subject: 0 hours, 56 minutes\n",
      "Average saccade duration per subject: 0 hours, 13 minutes\n",
      "Average blink duration per subject: 0 hours, 3 minutes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "root_dir = \"/gpfs1/pi/djangraw/mindless_reading/ROAMM/subject_ml_data/\"\n",
    "df = pd.read_csv(os.path.join(root_dir, \"subject_sample_counts_summary.csv\"))\n",
    "\n",
    "# get total samples across all subjects\n",
    "total_samples = df['total_sample'].sum()\n",
    "first_pass_samples = df['first_pass_sample'].sum()\n",
    "mind_wandering_samples = df['mind_wandering_sample'].sum()  \n",
    "fixation_samples = df['fixation_sample'].sum()\n",
    "saccade_samples = df['saccade_sample'].sum()\n",
    "blink_samples = df['blink_sample'].sum()\n",
    "\n",
    "# turn this into hours and minutes\n",
    "def format_duration(samples, sample_rate=256):\n",
    "    total_seconds = samples / sample_rate\n",
    "    hours = int(total_seconds // 3600)\n",
    "    minutes = int((total_seconds % 3600) // 60)\n",
    "    return f\"{hours} hours, {minutes} minutes\"\n",
    "\n",
    "print(\"Total samples across all subjects:\", total_samples)\n",
    "print(\"Total duration:\", format_duration(total_samples))\n",
    "print(\"First-pass reading samples across all subjects:\", first_pass_samples)\n",
    "print(\"First-pass reading duration:\", format_duration(first_pass_samples))\n",
    "print(\"Mind-wandering samples across all subjects:\", mind_wandering_samples)\n",
    "print(\"Mind-wandering duration:\", format_duration(mind_wandering_samples))\n",
    "print(\"Fixation samples across all subjects:\", fixation_samples)\n",
    "print(\"Fixation duration:\", format_duration(fixation_samples))\n",
    "print(\"Saccade samples across all subjects:\", saccade_samples)\n",
    "print(\"Saccade duration:\", format_duration(saccade_samples))\n",
    "print(\"Blink samples across all subjects:\", blink_samples)\n",
    "print(\"Blink duration:\", format_duration(blink_samples))\n",
    "\n",
    "# get subject average time\n",
    "print(\"Average total duration per subject:\", format_duration(df['total_sample'].mean()))\n",
    "print(\"Average first-pass reading duration per subject:\", format_duration(df['first_pass_sample'].mean()))\n",
    "print(\"Average mind-wandering duration per subject:\", format_duration(df['mind_wandering_sample'].mean()))\n",
    "print(\"Average fixation duration per subject:\", format_duration(df['fixation_sample'].mean()))\n",
    "print(\"Average saccade duration per subject:\", format_duration(df['saccade_sample'].mean()))\n",
    "print(\"Average blink duration per subject:\", format_duration(df['blink_sample'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c884d98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fp1', 'AF7', 'AF3', 'F1', 'F3', 'F5', 'F7', 'FT7', 'FC5', 'FC3', 'FC1', 'C1', 'C3', 'C5', 'T7', 'TP7', 'CP5', 'CP3', 'CP1', 'P1', 'P3', 'P5', 'P7', 'P9', 'PO7', 'PO3', 'O1', 'Iz', 'Oz', 'POz', 'Pz', 'CPz', 'Fpz', 'Fp2', 'AF8', 'AF4', 'Afz', 'Fz', 'F2', 'F4', 'F6', 'F8', 'FT8', 'FC6', 'FC4', 'FC2', 'FCz', 'Cz', 'C2', 'C4', 'C6', 'T8', 'TP8', 'CP6', 'CP4', 'CP2', 'P2', 'P4', 'P6', 'P8', 'P10', 'PO8', 'PO4', 'O2', 'time', 'sfreq', 'first_pass_reading', 'page_num', 'page_start', 'page_end', 'page_dur', 'is_mw', 'mw_onset', 'mw_offset', 'mw_dur', 'run_num', 'story_name', 'is_fix', 'fix_L_eye', 'fix_L_tStart', 'fix_L_tEnd', 'fix_L_duration', 'fix_L_xAvg', 'fix_L_yAvg', 'fix_L_pupilAvg', 'fix_L_fixed_word', 'fix_L_fixed_word_key', 'fix_R_eye', 'fix_R_tStart', 'fix_R_tEnd', 'fix_R_duration', 'fix_R_xAvg', 'fix_R_yAvg', 'fix_R_pupilAvg', 'fix_R_fixed_word', 'fix_R_fixed_word_key', 'is_blink', 'blink_L_eye', 'blink_L_tStart', 'blink_L_tEnd', 'blink_L_duration', 'blink_R_eye', 'blink_R_tStart', 'blink_R_tEnd', 'blink_R_duration', 'is_sacc', 'sacc_L_eye', 'sacc_L_tStart', 'sacc_L_tEnd', 'sacc_L_duration', 'sacc_L_xStart', 'sacc_L_yStart', 'sacc_L_xEnd', 'sacc_L_yEnd', 'sacc_L_ampDeg', 'sacc_L_vPeak', 'sacc_R_eye', 'sacc_R_tStart', 'sacc_R_tEnd', 'sacc_R_duration', 'sacc_R_xStart', 'sacc_R_yStart', 'sacc_R_xEnd', 'sacc_R_yEnd', 'sacc_R_ampDeg', 'sacc_R_vPeak', 'tSample', 'LX', 'LY', 'LPupil', 'RX', 'RY', 'RPupil', 'blink_interp_LX', 'blink_interp_LY', 'blink_interp_LPupil', 'blink_interp_RX', 'blink_interp_RY', 'blink_interp_RPupil']\n"
     ]
    }
   ],
   "source": [
    "print(df_data.columns.to_list())   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roamm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
