{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb1bd4e4",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bf63c2",
   "metadata": {},
   "source": [
    "## Individual process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f88933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import butter, sosfiltfilt, hilbert\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def fixation_bandpower_hilbert(df_page):\n",
    "    \"\"\"\n",
    "    Compute fixation-aligned EEG bandpower features using:\n",
    "      1) Band-pass filter per frequency band (SciPy butter + sosfiltfilt)\n",
    "      2) Hilbert transform to obtain analytic signal\n",
    "      3) Instantaneous bandpower time series = |analytic|^2 (log10 transformed)\n",
    "      4) Aggregate bandpower within each fixation window using sample indices\n",
    "\n",
    "    Expected input `df_page`:\n",
    "      - Rows are time samples, ordered in time)\n",
    "      - First 64 columns are EEG channel values (continuous EEG samples)\n",
    "      - Columns include:\n",
    "          sfreq (constant per run/page is fine),\n",
    "          fix_R_tStart, fix_R_tEnd,\n",
    "          is_mw,\n",
    "          fix_R_fixed_word, fix_R_fixed_word_key,\n",
    "          sentence_id, sentence\n",
    "\n",
    "    Returns:\n",
    "      DataFrame with one row per fixation interval and columns:\n",
    "        {ch}_{band} for 64*8 features + metadata cols.\n",
    "    \"\"\"\n",
    "\n",
    "    bands = {\n",
    "        \"theta1\": (4.0, 6.0),\n",
    "        \"theta2\": (6.5, 8.0),\n",
    "        \"alpha1\": (8.5, 10.0),\n",
    "        \"alpha2\": (10.5, 13.0),\n",
    "        \"beta1\":  (13.5, 18.0),\n",
    "        \"beta2\":  (18.5, 30.0),\n",
    "        \"gamma1\": (30.5, 40.0),\n",
    "        \"gamma2\": (40.0, 49.5),\n",
    "    }\n",
    "\n",
    "    # ---------------------------\n",
    "    # EEG: (n_ch, n_times)\n",
    "    # ---------------------------\n",
    "    eeg = df_page.iloc[:, :64].to_numpy(dtype=np.float64).T\n",
    "    n_ch, n_times = eeg.shape\n",
    "\n",
    "    ch_names = df_page.columns.values[:64].tolist()\n",
    "    sfreq = float(df_page[\"sfreq\"].iloc[0])\n",
    "    nyq = sfreq / 2.0\n",
    "\n",
    "    # ---------------------------\n",
    "    # Build fixation index table: one row per fixation interval\n",
    "    # start_idx/end_idx are *sample indices* (row indices) within df_page\n",
    "    # ---------------------------\n",
    "    df_fix_idx = (\n",
    "        df_page.dropna(subset=[\n",
    "            \"fix_R_tStart\", \"fix_R_tEnd\",\n",
    "            \"is_mw\", \"fix_R_fixed_word\", \"fix_R_fixed_word_key\",\n",
    "            \"sentence_id\", \"sentence\",\n",
    "        ])\n",
    "        .reset_index()  # creates column \"index\" = original sample row index\n",
    "        .groupby([\"fix_R_tStart\", \"fix_R_tEnd\"], as_index=False)\n",
    "        .agg(\n",
    "            start_idx=(\"index\", \"min\"),\n",
    "            end_idx=(\"index\", \"max\"),\n",
    "            is_mw=(\"is_mw\", \"mean\"),\n",
    "            fix_R_fixed_word=(\"fix_R_fixed_word\", \"first\"),\n",
    "            fix_R_fixed_word_key=(\"fix_R_fixed_word_key\", \"first\"),\n",
    "            sentence_id=(\"sentence_id\", \"first\"),\n",
    "            sentence=(\"sentence\", \"first\"),\n",
    "        )\n",
    "        .sort_values([\"fix_R_tStart\", \"fix_R_tEnd\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    if df_fix_idx.empty:\n",
    "        # no fixations found on this page\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    start_samp = np.clip(df_fix_idx[\"start_idx\"].to_numpy(), 0, n_times - 1)\n",
    "    end_samp   = np.clip(df_fix_idx[\"end_idx\"].to_numpy(),   0, n_times - 1)\n",
    "\n",
    "    band_names = list(bands.keys())\n",
    "    band_ranges = [bands[b] for b in band_names]\n",
    "    n_fix = len(df_fix_idx)\n",
    "    n_bands = len(band_names)\n",
    "\n",
    "    feat = np.full((n_fix, n_ch, n_bands), np.nan, dtype=np.float64)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Filter + Hilbert per band, then aggregate per fixation\n",
    "    # ---------------------------\n",
    "    for bi, (fmin, fmax) in enumerate(band_ranges):\n",
    "        # SciPy band-pass (zero-phase)\n",
    "        sos = butter(\n",
    "            N=4,\n",
    "            Wn=[fmin / nyq, fmax / nyq],\n",
    "            btype=\"bandpass\",\n",
    "            output=\"sos\",\n",
    "        )\n",
    "        x_filt = sosfiltfilt(sos, eeg, axis=1)           # filter over time\n",
    "        x_analytic = hilbert(x_filt, axis=1)             # hilbert over time\n",
    "        power = np.abs(x_analytic) ** 2                  # (n_ch, n_times)\n",
    "        # power = np.log10(power + 1e-20)                  # log power\n",
    "\n",
    "        for i in range(n_fix):\n",
    "            s = int(start_samp[i])\n",
    "            e = int(end_samp[i])\n",
    "            if e < s:\n",
    "                continue\n",
    "            feat[i, :, bi] = np.nanmean(power[:, s:e+1], axis=1)\n",
    "\n",
    "    # Flatten features: (n_fix, n_ch*n_bands)\n",
    "    feat_flat = feat.reshape(n_fix, n_ch * n_bands)\n",
    "    columns = [f\"{ch}_{band}\" for ch in ch_names for band in band_names]\n",
    "    df_psd = pd.DataFrame(feat_flat, columns=columns)\n",
    "\n",
    "    # Attach fixation metadata\n",
    "    df_psd = pd.concat([df_psd, df_fix_idx[[\n",
    "        \"is_mw\",\n",
    "        \"fix_R_fixed_word\",\n",
    "        \"fix_R_fixed_word_key\",\n",
    "        \"sentence_id\",\n",
    "        \"sentence\",\n",
    "    ]].reset_index(drop=True)], axis=1)\n",
    "\n",
    "    return df_psd\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Main processing\n",
    "# ==========================\n",
    "data_root = \"/gpfs1/pi/anon/mindless_reading/data\"\n",
    "coords_root = \"/gpfs1/pi/anon/hsun11/roamm_ml/res\"\n",
    "\n",
    "all_subjects = sorted(\n",
    "    d for d in os.listdir(data_root)\n",
    "    if d.startswith(\"s\") and os.path.isdir(os.path.join(data_root, d))\n",
    ")\n",
    "\n",
    "for subject_id in all_subjects:\n",
    "    print(f\"Processing subject {subject_id}...\")\n",
    "    ml_data_dir = os.path.join(data_root, subject_id, \"ml_data\")\n",
    "    save_dir = os.path.join(ml_data_dir, \"eeg2text_data\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    pkl_files = sorted([f for f in os.listdir(ml_data_dir) if f.endswith(\".pkl\")])\n",
    "\n",
    "    # make sure each subject has 5 runs of data\n",
    "    if len(pkl_files) != 5:\n",
    "        raise ValueError(f\"Subject {subject_id} has {len(pkl_files)} runs instead of 5\")\n",
    "\n",
    "    subject_rows = []  # collect per-page fixation-level dfs, concat once\n",
    "\n",
    "    for pkl_file in tqdm(pkl_files, desc=f\"{subject_id} runs\", unit=\"run\", leave=False):\n",
    "        df = pd.read_pickle(os.path.join(ml_data_dir, pkl_file))\n",
    "\n",
    "        # Filter: first pass reading only\n",
    "        if \"first_pass_reading\" in df.columns:\n",
    "            df = df[df[\"first_pass_reading\"] == 1].copy()\n",
    "\n",
    "        # convert bool col explicitly to avoid pandas warning\n",
    "        for col in ['is_blink', 'is_sacc', 'is_fix', 'is_mw', 'first_pass_reading']:\n",
    "            df[col] = df[col] == True\n",
    "\n",
    "        # Filter out samples 2 seconds before page end\n",
    "        if \"time\" in df.columns and \"page_end\" in df.columns:\n",
    "            df = df[df[\"time\"] < (df[\"page_end\"] - 2)].copy()\n",
    "\n",
    "        df[\"subject_id\"] = subject_id\n",
    "\n",
    "        # Sentence info merge (word_key)\n",
    "        story_name = df[\"story_name\"].iloc[0]\n",
    "        coord_path = os.path.join(coords_root, f\"{story_name}_coordinates.csv\")\n",
    "        df_coords = pd.read_csv(coord_path)\n",
    "\n",
    "        df = df.merge(\n",
    "            df_coords[[\"sentence_id\", \"sentence\", \"word_key\"]],\n",
    "            left_on=\"fix_R_fixed_word_key\",\n",
    "            right_on=\"word_key\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "\n",
    "        # Process each page\n",
    "        pages = sorted(df[\"page_num\"].unique().tolist())\n",
    "        for page in pages:\n",
    "            df_page = df[df[\"page_num\"] == page].copy()\n",
    "            df_fix_eeg = fixation_bandpower_hilbert(df_page)\n",
    "            if df_fix_eeg.empty:\n",
    "                continue\n",
    "            # Add metadata columns\n",
    "            df_fix_eeg[\"page_num\"] = page\n",
    "            df_fix_eeg[\"story_name\"] = story_name\n",
    "            df_fix_eeg[\"subject_id\"] = subject_id\n",
    "            subject_rows.append(df_fix_eeg)\n",
    "\n",
    "    if len(subject_rows) == 0:\n",
    "        Warning(f\"No fixation data found for subject {subject_id}, saving empty file.\")\n",
    "\n",
    "    df_group = pd.concat(subject_rows, ignore_index=True)\n",
    "    out_path = os.path.join(save_dir, f\"{subject_id}_eeg2text_data.csv\")\n",
    "    df_group.to_csv(out_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6effe62c",
   "metadata": {},
   "source": [
    "## Merge subject files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e17d283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_root = \"/gpfs1/pi/anon/mindless_reading/data\"\n",
    "all_subjects = sorted(\n",
    "    d for d in os.listdir(data_root)\n",
    "    if d.startswith(\"s\") and os.path.isdir(os.path.join(data_root, d))\n",
    ")\n",
    "df_list = []\n",
    "\n",
    "for subject_id in all_subjects:\n",
    "    ml_data_dir = os.path.join(data_root, subject_id, \"ml_data\")\n",
    "    save_dir = os.path.join(ml_data_dir, \"eeg2text_data\")\n",
    "    infile = os.path.join(save_dir, f\"{subject_id}_eeg2text_data.csv\")\n",
    "\n",
    "    df = pd.read_csv(infile)\n",
    "\n",
    "    df[\"subject_id\"] = subject_id  # enforce subject_id is correct\n",
    "    df_list.append(df)\n",
    "\n",
    "if len(df_list) == 0:\n",
    "    raise RuntimeError(\"No subject EEG2Text files were loaded. Nothing to concatenate.\")\n",
    "\n",
    "df_all = pd.concat(df_list, ignore_index=True)\n",
    "out_file = os.path.join(data_root, \"all_subjects_eeg2text_data.csv\")\n",
    "df_all.to_csv(out_file, index=False)\n",
    "\n",
    "print(f\"\\nSaved merged dataset to:\\n  {out_file}\")\n",
    "print(f\"Total rows: {len(df_all):,}\")\n",
    "print(f\"Total subjects loaded: {df_all['subject_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b013a86",
   "metadata": {},
   "source": [
    "## Average subject fixations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d24cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_root = \"/gpfs1/pi/anon/mindless_reading/data\"\n",
    "df = pd.read_csv(os.path.join(data_root, \"all_subjects_eeg2text_data.csv\"))\n",
    "\n",
    "# 0/NaN -> False, >0 -> True\n",
    "df[\"is_mw\"] = pd.to_numeric(df[\"is_mw\"], errors=\"coerce\").fillna(0).gt(0)\n",
    "\n",
    "sentence_id_col = \"sentence_id\"\n",
    "sentence_col = \"sentence\"\n",
    "fix_key_col = \"fix_R_fixed_word_key\"\n",
    "\n",
    "meta_cols = {\n",
    "    sentence_id_col, sentence_col, fix_key_col,\n",
    "    \"fix_R_fixed_word\", \"is_mw\", \"page_num\", \"story_name\", \"subject_id\",\n",
    "}\n",
    "\n",
    "# EEG columns = numeric columns not in meta\n",
    "eeg_cols = [c for c in df.columns if c not in meta_cols and pd.api.types.is_numeric_dtype(df[c])]\n",
    "if len(eeg_cols) == 0:\n",
    "    raise ValueError(\"No numeric EEG feature columns found (after excluding meta columns).\")\n",
    "\n",
    "# Group by (sentence + word_key + MW) and average EEG features\n",
    "df_avg = (\n",
    "    df.dropna(subset=[sentence_id_col, sentence_col, fix_key_col, \"fix_R_fixed_word\", \"is_mw\"])\n",
    "      .groupby([sentence_id_col, sentence_col, fix_key_col, \"fix_R_fixed_word\", \"is_mw\"], as_index=False)[eeg_cols]\n",
    "      .mean()\n",
    ")\n",
    "\n",
    "# df_avg.to_csv(os.path.join(data_root, \"all_subjects_eeg2text_data_avg.csv\"), index=False)\n",
    "# print(\"Saved averaged EEG2Text dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d7872",
   "metadata": {},
   "source": [
    "## Average subject fixations (based on sample points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b92b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_root = \"/gpfs1/pi/anon/mindless_reading/data\"\n",
    "df = pd.read_csv(os.path.join(data_root, \"all_subjects_eeg2text_data.csv\"))\n",
    "\n",
    "# 0/NaN -> False, >0 -> True\n",
    "df[\"is_mw\"] = pd.to_numeric(df[\"is_mw\"], errors=\"coerce\").fillna(0).gt(0)\n",
    "\n",
    "key = \"fix_R_fixed_word_key\"\n",
    "seed = 42\n",
    "# drop rows without key (otherwise groupby gets messy)\n",
    "df = df.dropna(subset=[key]).reset_index(drop=True)\n",
    "\n",
    "df_mw = df[df[\"is_mw\"]].copy()\n",
    "df_nm = df[~df[\"is_mw\"]].copy()\n",
    "# MW counts per word_key\n",
    "mw_counts = df_mw[key].value_counts()\n",
    "\n",
    "# Only keep non-MW word_keys that exist in MW (so distribution matches)\n",
    "df_nm = df_nm[df_nm[key].isin(mw_counts.index)].copy()\n",
    "\n",
    "# For each word_key, sample non-MW up to MW count for that key\n",
    "def sample_nm_group(g: pd.DataFrame) -> pd.DataFrame:\n",
    "    k = g.name\n",
    "    n_target = int(mw_counts.get(k, 0))\n",
    "    if n_target <= 0:\n",
    "        return g.iloc[0:0]  # empty\n",
    "    if len(g) <= n_target:\n",
    "        return g\n",
    "    return g.sample(n=n_target, random_state=seed)\n",
    "\n",
    "df_nm_bal = (\n",
    "    df_nm.groupby(key, group_keys=False)\n",
    "         .apply(sample_nm_group)\n",
    "         .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df = pd.concat([df_mw, df_nm_bal], ignore_index=True)\n",
    "\n",
    "sentence_id_col = \"sentence_id\"\n",
    "sentence_col = \"sentence\"\n",
    "fix_key_col = \"fix_R_fixed_word_key\"\n",
    "\n",
    "meta_cols = {\n",
    "    sentence_id_col, sentence_col, fix_key_col,\n",
    "    \"fix_R_fixed_word\", \"is_mw\", \"page_num\", \"story_name\", \"subject_id\",\n",
    "}\n",
    "\n",
    "# EEG columns = numeric columns not in meta\n",
    "eeg_cols = [c for c in df.columns if c not in meta_cols and pd.api.types.is_numeric_dtype(df[c])]\n",
    "if len(eeg_cols) == 0:\n",
    "    raise ValueError(\"No numeric EEG feature columns found (after excluding meta columns).\")\n",
    "\n",
    "# Group by (sentence + word_key + MW) and average EEG features\n",
    "df_avg = (\n",
    "    df.dropna(subset=[sentence_id_col, sentence_col, fix_key_col, \"fix_R_fixed_word\", \"is_mw\"])\n",
    "      .groupby([sentence_id_col, sentence_col, fix_key_col, \"fix_R_fixed_word\", \"is_mw\"], as_index=False)[eeg_cols]\n",
    "      .mean()\n",
    ")\n",
    "\n",
    "df_avg.to_csv(os.path.join(data_root, \"all_subjects_eeg2text_data_weightedavg.csv\"), index=False)\n",
    "print(\"Saved averaged EEG2Text dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99913156",
   "metadata": {},
   "source": [
    "# Examine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8c9ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "seed = 42\n",
    "csv_path = \"/gpfs1/pi/anon/mindless_reading/data/all_subjects_eeg2text_data.csv\"\n",
    "\n",
    "sentence_col = \"sentence\"\n",
    "\n",
    "df_all = pd.read_csv(csv_path)\n",
    "\n",
    "# =========================\n",
    "# ALL\n",
    "# =========================\n",
    "df = df_all.dropna(subset=[sentence_col]).reset_index(drop=True)\n",
    "\n",
    "subject_id = df['subject_id'].unique().tolist()\n",
    "\n",
    "sent_texts = df[sentence_col].unique().tolist()\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(sent_texts)\n",
    "\n",
    "n = len(sent_texts)\n",
    "n_train = int(round(0.8 * n))\n",
    "n_dev = int(round(0.1 * n))\n",
    "\n",
    "train_ids = set(sent_texts[:n_train])\n",
    "dev_ids   = set(sent_texts[n_train:n_train + n_dev])\n",
    "test_ids  = set(sent_texts[n_train + n_dev:])\n",
    "\n",
    "train_rows = df[sentence_col].isin(train_ids).sum()\n",
    "dev_rows   = df[sentence_col].isin(dev_ids).sum()\n",
    "test_rows  = df[sentence_col].isin(test_ids).sum()\n",
    "\n",
    "print(\"\\n=== ALL ===\")\n",
    "print(f\"Sentences total: {n} | train/dev/test: {n_train}/{n_dev}/{n - n_train - n_dev}\")\n",
    "print(f\"Rows total: {len(df)} | train/dev/test: {train_rows}/{dev_rows}/{test_rows}\")\n",
    "\n",
    "if \"is_mw\" in df.columns:\n",
    "    print(f\"MW rate overall: {df['is_mw'].mean():.3f}\")\n",
    "    print(\n",
    "        \"MW rate train/dev/test: \"\n",
    "        f\"{df.loc[df[sentence_col].isin(train_ids), 'is_mw'].mean():.3f}/\"\n",
    "        f\"{df.loc[df[sentence_col].isin(dev_ids), 'is_mw'].mean():.3f}/\"\n",
    "        f\"{df.loc[df[sentence_col].isin(test_ids), 'is_mw'].mean():.3f}\"\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# NR (is_mw == 0)\n",
    "# =========================\n",
    "df = df_all[df_all[\"is_mw\"] == 0].dropna(subset=[sentence_col]).reset_index(drop=True)\n",
    "\n",
    "sent_texts = df[sentence_col].unique().tolist()\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(sent_texts)\n",
    "\n",
    "n = len(sent_texts)\n",
    "n_train = int(round(0.8 * n))\n",
    "n_dev = int(round(0.1 * n))\n",
    "\n",
    "train_ids = set(sent_texts[:n_train])\n",
    "dev_ids   = set(sent_texts[n_train:n_train + n_dev])\n",
    "test_ids  = set(sent_texts[n_train + n_dev:])\n",
    "\n",
    "train_rows = df[sentence_col].isin(train_ids).sum()\n",
    "dev_rows   = df[sentence_col].isin(dev_ids).sum()\n",
    "test_rows  = df[sentence_col].isin(test_ids).sum()\n",
    "\n",
    "print(\"\\n=== NR (is_mw == 0) ===\")\n",
    "print(f\"Sentences total: {n} | train/dev/test: {n_train}/{n_dev}/{n - n_train - n_dev}\")\n",
    "print(f\"Rows total: {len(df)} | train/dev/test: {train_rows}/{dev_rows}/{test_rows}\")\n",
    "\n",
    "# =========================\n",
    "# MW (is_mw != 0)\n",
    "# =========================\n",
    "df = df_all[df_all[\"is_mw\"] != 0].dropna(subset=[sentence_col]).reset_index(drop=True)\n",
    "\n",
    "sent_texts = df[sentence_col].unique().tolist()\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(sent_texts)\n",
    "\n",
    "n = len(sent_texts)\n",
    "n_train = int(round(0.8 * n))\n",
    "n_dev = int(round(0.1 * n))\n",
    "\n",
    "train_ids = set(sent_texts[:n_train])\n",
    "dev_ids   = set(sent_texts[n_train:n_train + n_dev])\n",
    "test_ids  = set(sent_texts[n_train + n_dev:])\n",
    "\n",
    "train_rows = df[sentence_col].isin(train_ids).sum()\n",
    "dev_rows   = df[sentence_col].isin(dev_ids).sum()\n",
    "test_rows  = df[sentence_col].isin(test_ids).sum()\n",
    "\n",
    "print(\"\\n=== MW (is_mw != 0) ===\")\n",
    "print(f\"Sentences total: {n} | train/dev/test: {n_train}/{n_dev}/{n - n_train - n_dev}\")\n",
    "print(f\"Rows total: {len(df)} | train/dev/test: {train_rows}/{dev_rows}/{test_rows}\")\n",
    "print(\n",
    "    \"MW rate train/dev/test (should be 1.000/1.000/1.000): \"\n",
    "    f\"{df.loc[df[sentence_col].isin(train_ids), 'is_mw'].mean():.3f}/\"\n",
    "    f\"{df.loc[df[sentence_col].isin(dev_ids), 'is_mw'].mean():.3f}/\"\n",
    "    f\"{df.loc[df[sentence_col].isin(test_ids), 'is_mw'].mean():.3f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0c8a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "seed = 42\n",
    "csv_path = \"/gpfs1/pi/anon/mindless_reading/data/all_subjects_eeg2text_data.csv\"\n",
    "out_csv = \"/gpfs1/pi/anon/mindless_reading/data/eeg2text_split_stats_by_subject.csv\"\n",
    "\n",
    "sentence_col = \"sentence\"\n",
    "\n",
    "df_all = pd.read_csv(csv_path)\n",
    "\n",
    "# (optional but recommended) drop rows with missing sentence once, up front\n",
    "df_all = df_all.dropna(subset=[sentence_col]).reset_index(drop=True)\n",
    "\n",
    "rows_out = []\n",
    "\n",
    "all_subjects = sorted(df_all[\"subject_id\"].dropna().unique().tolist())\n",
    "\n",
    "for subject_id in all_subjects:\n",
    "    df_sub_all = df_all[df_all[\"subject_id\"] == subject_id].copy().reset_index(drop=True)\n",
    "\n",
    "    for label, df_sub in [\n",
    "        (\"all\", df_sub_all),\n",
    "        (\"nr\", df_sub_all[df_sub_all[\"is_mw\"] == 0].copy()),\n",
    "        (\"mw\", df_sub_all[df_sub_all[\"is_mw\"] != 0].copy()),\n",
    "    ]:\n",
    "        df_sub = df_sub.dropna(subset=[sentence_col]).reset_index(drop=True)\n",
    "\n",
    "        # if a subject has no rows for this subset, record zeros\n",
    "        if len(df_sub) == 0:\n",
    "            rows_out.append({\n",
    "                \"subject_id\": subject_id,\n",
    "                \"subset\": label,\n",
    "                \"n_sentences_total\": 0,\n",
    "                \"n_sentences_train\": 0,\n",
    "                \"n_sentences_dev\": 0,\n",
    "                \"n_sentences_test\": 0,\n",
    "                \"n_rows_total\": 0,\n",
    "                \"n_rows_train\": 0,\n",
    "                \"n_rows_dev\": 0,\n",
    "                \"n_rows_test\": 0,\n",
    "                \"mw_rate_overall\": np.nan,\n",
    "                \"mw_rate_train\": np.nan,\n",
    "                \"mw_rate_dev\": np.nan,\n",
    "                \"mw_rate_test\": np.nan,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # split by unique sentence text\n",
    "        sent_texts = df_sub[sentence_col].unique().tolist()\n",
    "        rng = np.random.RandomState(seed)\n",
    "        rng.shuffle(sent_texts)\n",
    "\n",
    "        n = len(sent_texts)\n",
    "        n_train = int(round(0.8 * n))\n",
    "        n_dev = int(round(0.1 * n))\n",
    "\n",
    "        train_ids = set(sent_texts[:n_train])\n",
    "        dev_ids   = set(sent_texts[n_train:n_train + n_dev])\n",
    "        test_ids  = set(sent_texts[n_train + n_dev:])\n",
    "\n",
    "        train_mask = df_sub[sentence_col].isin(train_ids)\n",
    "        dev_mask   = df_sub[sentence_col].isin(dev_ids)\n",
    "        test_mask  = df_sub[sentence_col].isin(test_ids)\n",
    "\n",
    "        # MW rates (only meaningful for subset=all; nr/mw will be 0/1)\n",
    "        if \"is_mw\" in df_sub.columns:\n",
    "            mw_rate_overall = float(df_sub[\"is_mw\"].mean())\n",
    "            mw_rate_train = float(df_sub.loc[train_mask, \"is_mw\"].mean()) if train_mask.any() else np.nan\n",
    "            mw_rate_dev   = float(df_sub.loc[dev_mask, \"is_mw\"].mean()) if dev_mask.any() else np.nan\n",
    "            mw_rate_test  = float(df_sub.loc[test_mask, \"is_mw\"].mean()) if test_mask.any() else np.nan\n",
    "        else:\n",
    "            mw_rate_overall = mw_rate_train = mw_rate_dev = mw_rate_test = np.nan\n",
    "\n",
    "        rows_out.append({\n",
    "            \"subject_id\": subject_id,\n",
    "            \"subset\": label,\n",
    "            \"n_sentences_total\": n,\n",
    "            \"n_sentences_train\": n_train,\n",
    "            \"n_sentences_dev\": n_dev,\n",
    "            \"n_sentences_test\": n - n_train - n_dev,\n",
    "            \"n_rows_total\": len(df_sub),\n",
    "            \"n_rows_train\": int(train_mask.sum()),\n",
    "            \"n_rows_dev\": int(dev_mask.sum()),\n",
    "            \"n_rows_test\": int(test_mask.sum()),\n",
    "            \"mw_rate_overall\": mw_rate_overall,\n",
    "            \"mw_rate_train\": mw_rate_train,\n",
    "            \"mw_rate_dev\": mw_rate_dev,\n",
    "            \"mw_rate_test\": mw_rate_test,\n",
    "        })\n",
    "\n",
    "df_stats = pd.DataFrame(rows_out)\n",
    "df_stats.to_csv(out_csv, index=False)\n",
    "\n",
    "print(f\"Saved stats to: {out_csv}\")\n",
    "print(df_stats.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819fecfe",
   "metadata": {},
   "source": [
    "# Plot EEG and eye-tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cbe462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def load_ml_data(subject_id, run_number, data_root=r\"/gpfs1/pi/anon/mindless_reading/data\"):\n",
    "    \"\"\"\n",
    "    Load ML-ready dataset with synchronized EEG and eye-tracking data\n",
    "    \n",
    "    Parameters:\n",
    "    subject_id (str): Subject ID (e.g., 's10014')\n",
    "    run_number (int): Run number (1-5)\n",
    "    data_root (str): Root directory containing subject data\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Combined EEG and eye-tracking data\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(data_root, subject_id, 'ml_data', f'{subject_id}_run{run_number}_ml_data.pkl')\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        print(\"Available files:\")\n",
    "        ml_dir = os.path.join(data_root, subject_id, 'ml_data')\n",
    "        if os.path.exists(ml_dir):\n",
    "            files = os.listdir(ml_dir)\n",
    "            for f in files:\n",
    "                print(f\"  {f}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Loading data from: {file_path}\")\n",
    "    df = pd.read_pickle(file_path)\n",
    "    print(f\"Loaded {len(df)} samples with {len(df.columns)} columns\")\n",
    "    print(f\"Time range: {df['time'].min():.2f} - {df['time'].max():.2f} seconds\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_representative_channels(all_channels, mode='comprehensive'):\n",
    "    \"\"\"\n",
    "    Select representative EEG channels from different brain regions\n",
    "    \n",
    "    Parameters:\n",
    "    all_channels (list): List of all available EEG channel names\n",
    "    mode (str): Selection mode - 'comprehensive', 'reading', 'attention', or 'minimal'\n",
    "    \n",
    "    Returns:\n",
    "    list: Selected representative channels with region labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define representative channels by brain region (standard 10-20 system)\n",
    "    channel_regions = {\n",
    "        # Frontal regions (executive control, attention)\n",
    "        'Frontal': ['Fp1', 'Fp2', 'F3', 'F4', 'F7', 'F8', 'Fz', 'FC1', 'FC2'],\n",
    "        \n",
    "        # Central regions (sensorimotor)\n",
    "        'Central': ['C3', 'C4', 'Cz', 'CP1', 'CP2'],\n",
    "        \n",
    "        # Parietal regions (attention, spatial processing)\n",
    "        'Parietal': ['P3', 'P4', 'Pz', 'P7', 'P8'],\n",
    "        \n",
    "        # Temporal regions (language, memory)\n",
    "        'Temporal': ['T7', 'T8', 'TP9', 'TP10'],\n",
    "        \n",
    "        # Occipital regions (visual processing)\n",
    "        'Occipital': ['O1', 'O2', 'Oz'],\n",
    "        \n",
    "        # Additional channels for comprehensive coverage\n",
    "        'Additional': ['AF3', 'AF4', 'FC5', 'FC6', 'CP5', 'CP6', 'PO3', 'PO4']\n",
    "    }\n",
    "    \n",
    "    # Different selection modes\n",
    "    if mode == 'minimal':\n",
    "        priority_channels = ['Fz', 'Cz', 'Pz', 'Oz', 'F3', 'F4', 'P3', 'P4']\n",
    "    elif mode == 'reading':\n",
    "        # Focus on language and reading-related areas\n",
    "        priority_channels = ['F3', 'F4', 'T7', 'T8', 'P3', 'P4', 'O1', 'O2', \n",
    "                           'Fz', 'Pz', 'C3', 'C4', 'TP9', 'TP10']\n",
    "    elif mode == 'attention':\n",
    "        # Focus on attention and executive control\n",
    "        priority_channels = ['F3', 'F4', 'Fz', 'P3', 'P4', 'Pz', 'C3', 'C4', \n",
    "                           'FC1', 'FC2', 'CP1', 'CP2']\n",
    "    else:  # comprehensive\n",
    "        priority_channels = ['Fz', 'Cz', 'Pz', 'Oz',  # Midline\n",
    "                           'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2',  # Bilateral\n",
    "                           'T7', 'T8', 'F7', 'F8',  # Temporal/lateral\n",
    "                           'FC1', 'FC2', 'CP1', 'CP2']  # Additional coverage\n",
    "    \n",
    "    # Find available channels from priority list\n",
    "    selected_channels = []\n",
    "    channel_labels = []\n",
    "    \n",
    "    for channel in priority_channels:\n",
    "        if channel in all_channels:\n",
    "            selected_channels.append(channel)\n",
    "            # Add region label\n",
    "            region = 'Unknown'\n",
    "            for reg_name, reg_channels in channel_regions.items():\n",
    "                if channel in reg_channels:\n",
    "                    region = reg_name\n",
    "                    break\n",
    "            channel_labels.append(f\"{channel} ({region})\")\n",
    "    \n",
    "    # If we don't have enough representative channels, fill with available ones\n",
    "    if len(selected_channels) < 8:\n",
    "        remaining_channels = [ch for ch in all_channels if ch not in selected_channels]\n",
    "        needed = min(16 - len(selected_channels), len(remaining_channels))\n",
    "        for i in range(needed):\n",
    "            selected_channels.append(remaining_channels[i])\n",
    "            channel_labels.append(f\"{remaining_channels[i]} (Other)\")\n",
    "    \n",
    "    return selected_channels, channel_labels\n",
    "\n",
    "def plot_eeg_eyetracking_sync(df_data, time_start=100, time_end=110, \n",
    "                              eeg_channels=None, channel_mode='comprehensive'):\n",
    "    \"\"\"\n",
    "    Create synchronized plots of EEG and eye-tracking data with representative channels\n",
    "    \n",
    "    Parameters:\n",
    "    df_data (DataFrame): Combined EEG and eye-tracking data\n",
    "    time_start (float): Start time in seconds\n",
    "    time_end (float): End time in seconds\n",
    "    eeg_channels (list): List of EEG channel names (if None, selects representative channels)\n",
    "    channel_mode (str): Channel selection mode - 'comprehensive', 'reading', 'attention', or 'minimal'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter data for time window\n",
    "    mask = (df_data['time'] >= time_start) & (df_data['time'] <= time_end)\n",
    "    df_plot = df_data[mask].copy()\n",
    "    \n",
    "    if len(df_plot) == 0:\n",
    "        print(f\"No data found in time range {time_start}-{time_end}\")\n",
    "        return\n",
    "    \n",
    "    time = np.array(df_plot['time'])\n",
    "    \n",
    "    # Get all available EEG channel names\n",
    "    non_eeg_cols = ['time', 'sfreq', 'run_num', 'story_name', 'first_pass_reading', \n",
    "                   'page_num', 'page_start', 'page_end', 'page_dur', 'is_mw', \n",
    "                   'mw_onset', 'mw_offset', 'mw_dur']\n",
    "    all_eeg_channels = [col for col in df_plot.columns if col not in non_eeg_cols \n",
    "                       and not col.startswith(('is_', 'fix_', 'blink_', 'sacc_', 'tSample', 'LX', 'LY', 'RX', 'RY'))]\n",
    "    \n",
    "    # Select representative channels\n",
    "    if eeg_channels is None:\n",
    "        eeg_channels, channel_labels = get_representative_channels(all_eeg_channels, mode=channel_mode)\n",
    "    else:\n",
    "        channel_labels = [f\"{ch} (Custom)\" for ch in eeg_channels]\n",
    "    \n",
    "    print(f\"Plotting {len(eeg_channels)} representative EEG channels from {time_start} to {time_end} seconds\")\n",
    "    print(f\"Channel selection mode: {channel_mode}\")\n",
    "    print(f\"Selected channels: {', '.join(eeg_channels)}\")\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(12, 16))\n",
    "    \n",
    "    # EEG plot (top subplot)\n",
    "    ax1 = plt.subplot(3, 1, 1)\n",
    "    \n",
    "    # Get EEG data\n",
    "    eeg_data = np.array(df_plot[eeg_channels])\n",
    "    \n",
    "    # Vertical spacing between channels\n",
    "    channel_spacing = np.std(eeg_data) * 3\n",
    "    \n",
    "    # Group channels by brain region in reverse anatomical order (occipital to frontal)\n",
    "    region_order = ['Occipital', 'Temporal', 'Parietal', 'Central', 'Frontal', 'Additional', 'Other', 'Custom']\n",
    "    region_colors = {\n",
    "        'Frontal': 'blue',\n",
    "        'Central': 'green', \n",
    "        'Parietal': 'orange',\n",
    "        'Temporal': 'red',\n",
    "        'Occipital': 'purple',\n",
    "        'Additional': 'brown',\n",
    "        'Other': 'gray',\n",
    "        'Custom': 'black'\n",
    "    }\n",
    "    \n",
    "    # Group channels by region\n",
    "    channels_by_region = {}\n",
    "    for i, label in enumerate(channel_labels):\n",
    "        region = label.split('(')[1].split(')')[0] if '(' in label else 'Other'\n",
    "        if region not in channels_by_region:\n",
    "            channels_by_region[region] = []\n",
    "        channels_by_region[region].append((eeg_channels[i], label, i))\n",
    "    \n",
    "    # Plot channels grouped by region in anatomical order\n",
    "    current_y_offset = 0\n",
    "    region_boundaries = []\n",
    "    \n",
    "    for region in region_order:\n",
    "        if region in channels_by_region:\n",
    "            color = region_colors[region]\n",
    "            region_start = current_y_offset\n",
    "            \n",
    "            # Plot all channels in this region\n",
    "            for channel_name, label, orig_idx in channels_by_region[region]:\n",
    "                ax1.plot(time, eeg_data[:, orig_idx] + current_y_offset, \n",
    "                        color=color, linewidth=0.8, alpha=0.8)\n",
    "                \n",
    "                # Add original channel names on the left inside\n",
    "                ax1.text(time_start - (time_end - time_start) * 0.02, current_y_offset, channel_name, \n",
    "                        verticalalignment='center', horizontalalignment='left',\n",
    "                        fontsize=8, color=color)\n",
    "                current_y_offset += channel_spacing\n",
    "            \n",
    "            # Add region separator and label\n",
    "            region_end = current_y_offset - channel_spacing/2\n",
    "            region_boundaries.append((region, region_start, region_end, color))\n",
    "            \n",
    "            # Add some extra space between regions\n",
    "            current_y_offset += channel_spacing * 0.3\n",
    "    \n",
    "    # Add region labels on the right/outside\n",
    "    for region, start_y, end_y, color in region_boundaries:\n",
    "        region_center = (start_y + end_y) / 2\n",
    "        ax1.text(time_end + (time_end - time_start) * 0.01, region_center, region, \n",
    "                verticalalignment='center', horizontalalignment='left',\n",
    "                fontsize=10, fontweight='bold', color=color,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=color, alpha=0.2))\n",
    "    \n",
    "    # Set ylabel with consistent positioning from left edge\n",
    "    ax1.text(-0.05, 0.5, 'EEG Channels', \n",
    "             transform=ax1.transAxes, fontsize=12, fontweight='bold',\n",
    "             verticalalignment='center', horizontalalignment='center',\n",
    "             rotation=90)\n",
    "    # ax1.set_title(f'Synchronized EEG and Eye-Tracking Data ({time_end-time_start}s)', \n",
    "                #  fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylim(-channel_spacing, current_y_offset)\n",
    "    ax1.set_yticks([])\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(time_start, time_end)\n",
    "    \n",
    "    # Eye movement events (middle subplot)\n",
    "    ax2 = plt.subplot(3, 1, 2)\n",
    "    \n",
    "    # Plot fixations, saccades, and blinks\n",
    "    y_pos = 0\n",
    "    colors = ['green', 'orange', 'red']\n",
    "    labels = ['Fixation', 'Saccade', 'Blink']\n",
    "    \n",
    "    for event_type, color, label in zip(['is_fix', 'is_sacc', 'is_blink'], colors, labels):\n",
    "        if event_type in df_plot.columns:\n",
    "            event_data = df_plot[event_type].fillna(0)\n",
    "            # Create binary signal for visualization\n",
    "            event_signal = np.where(event_data > 0, y_pos + 0.4, y_pos)\n",
    "            ax2.fill_between(time, y_pos, event_signal, color=color, alpha=0.7, label=label)\n",
    "        y_pos += 1\n",
    "    \n",
    "    # Add fixated words as text underneath fixations in multiple lines\n",
    "    if 'fix_R_fixed_word' in df_plot.columns:\n",
    "        # Get fixation periods and their words\n",
    "        fixation_mask = df_plot['is_fix'].fillna(0) > 0\n",
    "        if fixation_mask.any():\n",
    "            # Find fixation start and end points\n",
    "            fixation_changes = np.diff(fixation_mask.astype(int))\n",
    "            fixation_starts = np.where(fixation_changes == 1)[0] + 1\n",
    "            fixation_ends = np.where(fixation_changes == -1)[0] + 1\n",
    "            \n",
    "            # Handle edge cases\n",
    "            if fixation_mask.iloc[0]:\n",
    "                fixation_starts = np.concatenate([[0], fixation_starts])\n",
    "            if fixation_mask.iloc[-1]:\n",
    "                fixation_ends = np.concatenate([fixation_ends, [len(fixation_mask)]])\n",
    "            \n",
    "            # Collect all fixation words with their positions\n",
    "            fixation_words = []\n",
    "            for start_idx, end_idx in zip(fixation_starts, fixation_ends):\n",
    "                if start_idx < len(df_plot) and end_idx <= len(df_plot):\n",
    "                    fixation_time = time[start_idx:end_idx]\n",
    "                    if len(fixation_time) > 0:\n",
    "                        mid_time = fixation_time[len(fixation_time)//2]\n",
    "                        start_time = fixation_time[0]\n",
    "                        end_time = fixation_time[-1]\n",
    "                        \n",
    "                        # Get the fixated word\n",
    "                        word_data = df_plot.iloc[start_idx:end_idx]['fix_R_fixed_word'].dropna()\n",
    "                        if not word_data.empty:\n",
    "                            word = str(word_data.iloc[0])\n",
    "                            if word != 'nan' and word != 'None' and word.strip():\n",
    "                                fixation_words.append({\n",
    "                                    'word': word,\n",
    "                                    'start_time': start_time,\n",
    "                                    'end_time': end_time,\n",
    "                                    'mid_time': mid_time\n",
    "                                })\n",
    "            \n",
    "            # Plot words in multiple lines to avoid collisions\n",
    "            if fixation_words:\n",
    "                # Sort by start time\n",
    "                fixation_words.sort(key=lambda x: x['start_time'])\n",
    "                \n",
    "                # Assign words to lines to minimize overlap\n",
    "                word_lines = []  # Each element is a list of words for that line\n",
    "                line_positions = [-1.5, -2.0, -2.5, -3.0]  # Y positions for word lines\n",
    "                \n",
    "                for word_info in fixation_words:\n",
    "                    word_placed = False\n",
    "                    \n",
    "                    # Try to place word in existing lines\n",
    "                    for line_idx, line in enumerate(word_lines):\n",
    "                        # Check if word overlaps with any word in this line\n",
    "                        overlap = False\n",
    "                        for existing_word in line:\n",
    "                            # Simple overlap check: if words are too close in time\n",
    "                            time_diff = abs(word_info['mid_time'] - existing_word['mid_time'])\n",
    "                            if time_diff < (time_end - time_start) * 0.05:  # 5% of time window\n",
    "                                overlap = True\n",
    "                                break\n",
    "                        \n",
    "                        if not overlap:\n",
    "                            line.append(word_info)\n",
    "                            word_placed = True\n",
    "                            break\n",
    "                    \n",
    "                    # If word doesn't fit in existing lines, create new line\n",
    "                    if not word_placed:\n",
    "                        if len(word_lines) < len(line_positions):\n",
    "                            word_lines.append([word_info])\n",
    "                        else:\n",
    "                            # If we've used all lines, add to the last line anyway\n",
    "                            word_lines[-1].append(word_info)\n",
    "                \n",
    "                # Plot the words\n",
    "                for line_idx, line in enumerate(word_lines):\n",
    "                    y_pos = line_positions[line_idx]\n",
    "                    for word_info in line:\n",
    "                        # Plot word text\n",
    "                        ax2.text(word_info['mid_time'], y_pos, word_info['word'],\n",
    "                                ha='center', va='center', fontsize=10,\n",
    "                                bbox=dict(boxstyle=\"round,pad=0.2\", facecolor='lightgreen', alpha=0.7),\n",
    "                                color='darkgreen', fontweight='bold')\n",
    "                        \n",
    "                        # Draw line connecting word to fixation\n",
    "                        ax2.plot([word_info['mid_time'], word_info['mid_time']], \n",
    "                                [y_pos + 0.15, 0.2], \n",
    "                                color='green', alpha=0.5, linewidth=1, linestyle='--')\n",
    "    \n",
    "    # Set ylabel with consistent positioning from left edge\n",
    "    ax2.text(-0.05, 0.5, 'Eye Events', \n",
    "             transform=ax2.transAxes, fontsize=12, fontweight='bold',\n",
    "             verticalalignment='center', horizontalalignment='center',\n",
    "             rotation=90)\n",
    "    ax2.set_ylim(-3.5, 3.5)  # Extended range to accommodate word lines\n",
    "    ax2.set_yticks([0, 1, 2])\n",
    "    ax2.set_yticklabels(['Fixation', 'Saccade', 'Blink'])\n",
    "    # move y-axis ticks and labels to the right\n",
    "    ax2.yaxis.tick_right()\n",
    "    ax2.yaxis.set_label_position(\"right\")\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xlim(time_start, time_end)\n",
    "    ax2.legend(loc='upper right')\n",
    "    \n",
    "    # Gaze position and pupil size (bottom subplot)\n",
    "    ax3 = plt.subplot(3, 1, 3)\n",
    "    \n",
    "    # Plot gaze position and pupil size\n",
    "    if 'blink_interp_RX' in df_plot.columns:\n",
    "        ax3_twin = ax3.twinx()\n",
    "        \n",
    "        # Gaze position\n",
    "        if 'blink_interp_RX' in df_plot.columns and 'blink_interp_RY' in df_plot.columns:\n",
    "            ax3.plot(time, df_plot['blink_interp_RX'], 'r-', label='Gaze X', alpha=0.8)\n",
    "            ax3.plot(time, df_plot['blink_interp_RY'], 'b-', label='Gaze Y', alpha=0.8)\n",
    "            # Set ylabel with consistent positioning from left edge\n",
    "            ax3.text(-0.05, 0.5, 'Gaze Position (pixels)', \n",
    "                     transform=ax3.transAxes, fontsize=12, fontweight='bold', color='black',\n",
    "                     verticalalignment='center', horizontalalignment='center',\n",
    "                     rotation=90)\n",
    "            ax3.legend(loc='upper left')\n",
    "        \n",
    "        # Pupil size\n",
    "        if 'blink_interp_RPupil' in df_plot.columns:\n",
    "            ax3_twin.plot(time, df_plot['blink_interp_RPupil'], 'grey', \n",
    "                         label='Interpolated Pupil Size', alpha=0.6, linewidth=3)\n",
    "            ax3_twin.set_ylabel('Interpolated Pupil Size', fontsize=12, fontweight='bold')\n",
    "            ax3_twin.legend(loc='upper right')\n",
    "    \n",
    "    ax3.set_xlabel('Time (seconds)', fontsize=12)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_xlim(time_start, time_end)\n",
    "    ax1.set_xticklabels([])\n",
    "    ax2.set_xticklabels([])\n",
    "    # Add mind-wandering periods if available\n",
    "    if 'is_mw' in df_plot.columns:\n",
    "        mw_data = df_plot['is_mw'].fillna(0)\n",
    "        mw_periods = np.where(mw_data > 0)[0]\n",
    "        if len(mw_periods) > 0:\n",
    "            # Highlight mind-wandering periods across all subplots\n",
    "            for ax in [ax1, ax2, ax3]:\n",
    "                for i in mw_periods:\n",
    "                    ax.axvline(time[i], color='red', alpha=0.3, linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n=== Data Summary ===\")\n",
    "    if 'is_fixation' in df_plot.columns:\n",
    "        fixation_time = df_plot['is_fixation'].fillna(0).sum() / df_plot['sfreq'].iloc[0]\n",
    "        print(f\"Fixation time: {fixation_time:.2f}s ({fixation_time/(time_end-time_start)*100:.1f}%)\")\n",
    "    \n",
    "    if 'is_saccade' in df_plot.columns:\n",
    "        saccade_time = df_plot['is_saccade'].fillna(0).sum() / df_plot['sfreq'].iloc[0]\n",
    "        print(f\"Saccade time: {saccade_time:.2f}s ({saccade_time/(time_end-time_start)*100:.1f}%)\")\n",
    "    \n",
    "    if 'is_blink' in df_plot.columns:\n",
    "        blink_time = df_plot['is_blink'].fillna(0).sum() / df_plot['sfreq'].iloc[0]\n",
    "        print(f\"Blink time: {blink_time:.2f}s ({blink_time/(time_end-time_start)*100:.1f}%)\")\n",
    "    \n",
    "    if 'is_mw' in df_plot.columns:\n",
    "        mw_time = df_plot['is_mw'].fillna(0).sum() / df_plot['sfreq'].iloc[0]\n",
    "        print(f\"Mind-wandering time: {mw_time:.2f}s ({mw_time/(time_end-time_start)*100:.1f}%)\")\n",
    "\n",
    "# Load and plot data\n",
    "subject_id = 's10110'\n",
    "run_number = 1\n",
    "\n",
    "# Load the data\n",
    "df_data = load_ml_data(subject_id, run_number)\n",
    "\n",
    "if df_data is not None:\n",
    "    # Plot synchronized EEG and eye-tracking data with representative channels\n",
    "    plot_eeg_eyetracking_sync(df_data, time_start=100, time_end=103, channel_mode='reading')\n",
    "    \n",
    "    # You can also try different channel selection modes:\n",
    "    # plot_eeg_eyetracking_sync(df_data, time_start=100, time_end=110, channel_mode='comprehensive')\n",
    "    # plot_eeg_eyetracking_sync(df_data, time_start=100, time_end=110, channel_mode='attention')\n",
    "    # plot_eeg_eyetracking_sync(df_data, time_start=100, time_end=110, channel_mode='minimal')\n",
    "else:\n",
    "    print(\"Could not load data. Please check the file path and ensure data exists.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roamm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
